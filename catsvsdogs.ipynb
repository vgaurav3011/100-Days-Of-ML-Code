{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "catsvsdogs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgaurav3011/100-Days-Of-ML-Code/blob/master/catsvsdogs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAQE9khvwHGb",
        "colab_type": "code",
        "outputId": "04728f33-e748-48fa-ce1e-a7a748f57b5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "itfrom google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /tmp/cats_and_dogs_filtered.zip\n",
        "\n",
        "\n",
        "!pip install bokeh\n",
        "from keras.callbacks import *\n",
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop,sgd\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from bokeh.plotting import figure, show\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "local_zip = '/tmp/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training cat pictures\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directory with our training dog pictures\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directory with our validation cat pictures\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directory with our validation dog pictures\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),#148\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),#74\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),#36\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),#17\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),#7\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),#5\n",
        "    tf.keras.layers.Conv2D(8, (3,3), activation='relu'),#3\n",
        "    tf.keras.layers.Conv2D(1, (3,3), activation='sigmoid'),#1\n",
        "    tf.keras.layers.Flatten(),\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# This code has changed. Now instead of the ImageGenerator just rescaling\n",
        "# the image, we also rotate and do other operations\n",
        "# Updated to do image augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=45,\n",
        "      zca_whitening=True,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=20,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(150, 150),\n",
        "        batch_size=20,\n",
        "        class_mode='binary')\n",
        "\n",
        "filepath=\"/content/drive/My Drive/Colab Notebooks/weights.{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath,monitor='val_acc', verbose=1,save_weights_only=False, save_best_only=True)  \n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--2020-01-16 18:55:47--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 2607:f8b0:4001:c14::80\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/tmp/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/tmp/cats_and_dogs_ 100%[===================>]  65.43M   204MB/s    in 0.3s    \n",
            "\n",
            "2020-01-16 18:55:47 (204 MB/s) - ‘/tmp/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh) (6.2.2)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh) (4.5.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh) (3.13)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.12.0)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.10.3)\n",
            "Requirement already satisfied: numpy>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (1.17.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from bokeh) (2.6.1)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.6/dist-packages (from bokeh) (20.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh) (1.1.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging>=16.8->bokeh) (2.4.6)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:336: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3o6OEtOyXiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cyclic Learning Rate\n",
        "class CyclicLR(Callback):\n",
        "  \n",
        "\n",
        "    def __init__(self, base_lr=0.00000005, max_lr=0.009, step_size=2000., mode='triangular',\n",
        "                 gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn == None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1/(2.**(x-1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma**(x)\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr != None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr != None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size != None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "        \n",
        "    def clr(self):\n",
        "        cycle = np.floor(1+self.clr_iterations/(2*self.step_size))\n",
        "        x = np.abs(self.clr_iterations/self.step_size - 2*cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())        \n",
        "            \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        \n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "\n",
        "        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "def find_lr(model, start_lr, end_lr):\n",
        "    finder = LRFinder(start_lr, end_lr, len(train_generator))\n",
        "    weights = model.get_weights()    \n",
        "    try:\n",
        "        history = model.fit_generator(\n",
        "            generator=train_generator,\n",
        "            validation_data=validation_generator,\n",
        "            epochs=1,\n",
        "            verbose=1,\n",
        "            callbacks=[finder],\n",
        "        )   \n",
        "    finally:    \n",
        "        model.set_weights(weights)    \n",
        "    return finder\n",
        "    \n",
        "class LRFinder(Callback):    \n",
        "    def __init__(self, start_lr=1e-5, end_lr=10, step_size=None, beta=.98):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.start_lr = start_lr\n",
        "        self.end_lr = end_lr\n",
        "        self.step_size = step_size\n",
        "        self.beta = beta\n",
        "        self.lr_mult = (end_lr/start_lr)**(1/step_size)\n",
        "        \n",
        "    def on_train_begin(self, logs=None):\n",
        "        self.best_loss = 1e9\n",
        "        self.avg_loss = 0\n",
        "        self.losses, self.smoothed_losses, self.lrs, self.iterations = [], [], [], []\n",
        "        self.iteration = 0\n",
        "        logs = logs or {}\n",
        "        K.set_value(self.model.optimizer.lr, self.start_lr)\n",
        "        \n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        loss = logs.get('loss')\n",
        "        self.iteration += 1\n",
        "        \n",
        "        self.avg_loss = self.beta * self.avg_loss + (1 - self.beta) * loss\n",
        "        smoothed_loss = self.avg_loss / (1 - self.beta**self.iteration)\n",
        "        \n",
        "        # Check if the loss is not exploding\n",
        "        if self.iteration>1 and smoothed_loss > self.best_loss * 4:\n",
        "            self.model.stop_training = True\n",
        "            return\n",
        "\n",
        "        if smoothed_loss < self.best_loss or self.iteration==1:\n",
        "            self.best_loss = smoothed_loss\n",
        "        \n",
        "        lr = self.start_lr * (self.lr_mult**self.iteration)\n",
        "        \n",
        "        self.losses.append(loss)\n",
        "        self.smoothed_losses.append(smoothed_loss)\n",
        "        self.lrs.append(lr)\n",
        "        self.iterations.append(self.iteration)        \n",
        "        \n",
        "        K.set_value(self.model.optimizer.lr, lr)  \n",
        "\n",
        "    def plot(self, lskip=10, rskip=10):\n",
        "        lrs = self.lrs[lskip:-rskip]\n",
        "        losses = self.smoothed_losses[lskip:-rskip]\n",
        "\n",
        "        output_notebook()\n",
        "        p = figure(title='Learning Rate Finder', x_axis_label='LR', y_axis_label='Loss')\n",
        "        p.line(lrs, losses)\n",
        "        show(p)\n",
        "        \n",
        "        best_idxs = np.argpartition(losses, 15)[:15]\n",
        "        best_lrs = np.take(lrs, best_idxs)\n",
        "        print(f\"Best LRs: {best_lrs}\")  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cejkA_NDwjh3",
        "colab_type": "code",
        "outputId": "13975c94-f392-415e-cb40-21d934f626e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 148, 148, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 148, 148, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 72, 72, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 72, 72, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 34, 34, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 34, 34, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 15, 15, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 15, 15, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 5, 5, 32)          36896     \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 3, 3, 8)           2312      \n",
            "_________________________________________________________________\n",
            "conv2d_13 (Conv2D)           (None, 1, 1, 1)           73        \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 281,521\n",
            "Trainable params: 280,817\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6U6yNm6wdoJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = RMSprop(lr=0.001)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\"acc\"])\n",
        "#finder = find_lr(model, 1e-7, 1e-2)\n",
        "#finder.plot()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylE7hzh_2HO3",
        "colab_type": "code",
        "outputId": "fb990b1d-2df8-4910-f5e7-f584b32e373b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2,callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.50200, saving model to /content/drive/My Drive/Colab Notebooks/weights.01-0.50.hdf5\n",
            "100/100 - 25s - loss: 0.7221 - acc: 0.5595 - val_loss: 0.7226 - val_acc: 0.5020\n",
            "Epoch 2/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.50200 to 0.51000, saving model to /content/drive/My Drive/Colab Notebooks/weights.02-0.51.hdf5\n",
            "100/100 - 16s - loss: 0.6902 - acc: 0.5670 - val_loss: 0.6938 - val_acc: 0.5100\n",
            "Epoch 3/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00003: val_acc improved from 0.51000 to 0.52000, saving model to /content/drive/My Drive/Colab Notebooks/weights.03-0.52.hdf5\n",
            "100/100 - 16s - loss: 0.6822 - acc: 0.5700 - val_loss: 0.7214 - val_acc: 0.5200\n",
            "Epoch 4/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00004: val_acc improved from 0.52000 to 0.57500, saving model to /content/drive/My Drive/Colab Notebooks/weights.04-0.57.hdf5\n",
            "100/100 - 17s - loss: 0.6701 - acc: 0.5865 - val_loss: 0.7650 - val_acc: 0.5750\n",
            "Epoch 5/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00005: val_acc improved from 0.57500 to 0.60000, saving model to /content/drive/My Drive/Colab Notebooks/weights.05-0.60.hdf5\n",
            "100/100 - 16s - loss: 0.6513 - acc: 0.6115 - val_loss: 0.6613 - val_acc: 0.6000\n",
            "Epoch 6/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00006: val_acc improved from 0.60000 to 0.60700, saving model to /content/drive/My Drive/Colab Notebooks/weights.06-0.61.hdf5\n",
            "100/100 - 16s - loss: 0.6418 - acc: 0.6205 - val_loss: 0.6608 - val_acc: 0.6070\n",
            "Epoch 7/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.60700\n",
            "100/100 - 16s - loss: 0.6425 - acc: 0.6095 - val_loss: 0.7539 - val_acc: 0.5550\n",
            "Epoch 8/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00008: val_acc improved from 0.60700 to 0.65000, saving model to /content/drive/My Drive/Colab Notebooks/weights.08-0.65.hdf5\n",
            "100/100 - 16s - loss: 0.6454 - acc: 0.6245 - val_loss: 0.6318 - val_acc: 0.6500\n",
            "Epoch 9/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00009: val_acc improved from 0.65000 to 0.65600, saving model to /content/drive/My Drive/Colab Notebooks/weights.09-0.66.hdf5\n",
            "100/100 - 17s - loss: 0.6269 - acc: 0.6375 - val_loss: 0.6183 - val_acc: 0.6560\n",
            "Epoch 10/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.65600\n",
            "100/100 - 16s - loss: 0.6268 - acc: 0.6450 - val_loss: 0.6853 - val_acc: 0.5870\n",
            "Epoch 11/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00011: val_acc improved from 0.65600 to 0.69000, saving model to /content/drive/My Drive/Colab Notebooks/weights.11-0.69.hdf5\n",
            "100/100 - 16s - loss: 0.6129 - acc: 0.6600 - val_loss: 0.5813 - val_acc: 0.6900\n",
            "Epoch 12/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.69000\n",
            "100/100 - 16s - loss: 0.5979 - acc: 0.6755 - val_loss: 0.7163 - val_acc: 0.6380\n",
            "Epoch 13/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00013: val_acc improved from 0.69000 to 0.69700, saving model to /content/drive/My Drive/Colab Notebooks/weights.13-0.70.hdf5\n",
            "100/100 - 16s - loss: 0.5834 - acc: 0.7025 - val_loss: 0.5851 - val_acc: 0.6970\n",
            "Epoch 14/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00014: val_acc improved from 0.69700 to 0.70300, saving model to /content/drive/My Drive/Colab Notebooks/weights.14-0.70.hdf5\n",
            "100/100 - 16s - loss: 0.5892 - acc: 0.6910 - val_loss: 0.5578 - val_acc: 0.7030\n",
            "Epoch 15/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.70300 to 0.71000, saving model to /content/drive/My Drive/Colab Notebooks/weights.15-0.71.hdf5\n",
            "100/100 - 16s - loss: 0.5681 - acc: 0.7165 - val_loss: 0.5593 - val_acc: 0.7100\n",
            "Epoch 16/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.71000\n",
            "100/100 - 16s - loss: 0.5710 - acc: 0.6980 - val_loss: 0.5794 - val_acc: 0.6780\n",
            "Epoch 17/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.71000 to 0.71200, saving model to /content/drive/My Drive/Colab Notebooks/weights.17-0.71.hdf5\n",
            "100/100 - 16s - loss: 0.5703 - acc: 0.7065 - val_loss: 0.5489 - val_acc: 0.7120\n",
            "Epoch 18/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00018: val_acc improved from 0.71200 to 0.71900, saving model to /content/drive/My Drive/Colab Notebooks/weights.18-0.72.hdf5\n",
            "100/100 - 16s - loss: 0.5552 - acc: 0.7160 - val_loss: 0.5816 - val_acc: 0.7190\n",
            "Epoch 19/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.71900\n",
            "100/100 - 16s - loss: 0.5450 - acc: 0.7185 - val_loss: 0.8394 - val_acc: 0.5500\n",
            "Epoch 20/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00020: val_acc improved from 0.71900 to 0.72200, saving model to /content/drive/My Drive/Colab Notebooks/weights.20-0.72.hdf5\n",
            "100/100 - 16s - loss: 0.5437 - acc: 0.7295 - val_loss: 0.5611 - val_acc: 0.7220\n",
            "Epoch 21/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00021: val_acc improved from 0.72200 to 0.76000, saving model to /content/drive/My Drive/Colab Notebooks/weights.21-0.76.hdf5\n",
            "100/100 - 16s - loss: 0.5452 - acc: 0.7260 - val_loss: 0.5068 - val_acc: 0.7600\n",
            "Epoch 22/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.76000\n",
            "100/100 - 16s - loss: 0.5404 - acc: 0.7255 - val_loss: 2.2527 - val_acc: 0.5290\n",
            "Epoch 23/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00023: val_acc did not improve from 0.76000\n",
            "100/100 - 16s - loss: 0.5249 - acc: 0.7405 - val_loss: 0.5171 - val_acc: 0.7500\n",
            "Epoch 24/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.76000\n",
            "100/100 - 16s - loss: 0.5169 - acc: 0.7440 - val_loss: 0.6512 - val_acc: 0.6520\n",
            "Epoch 25/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00025: val_acc improved from 0.76000 to 0.77400, saving model to /content/drive/My Drive/Colab Notebooks/weights.25-0.77.hdf5\n",
            "100/100 - 16s - loss: 0.5123 - acc: 0.7565 - val_loss: 0.5035 - val_acc: 0.7740\n",
            "Epoch 26/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.77400\n",
            "100/100 - 16s - loss: 0.5071 - acc: 0.7545 - val_loss: 0.5980 - val_acc: 0.6990\n",
            "Epoch 27/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00027: val_acc did not improve from 0.77400\n",
            "100/100 - 16s - loss: 0.5066 - acc: 0.7530 - val_loss: 0.5254 - val_acc: 0.7240\n",
            "Epoch 28/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.77400\n",
            "100/100 - 16s - loss: 0.4875 - acc: 0.7640 - val_loss: 0.4899 - val_acc: 0.7630\n",
            "Epoch 29/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.77400\n",
            "100/100 - 16s - loss: 0.4783 - acc: 0.7780 - val_loss: 0.7292 - val_acc: 0.6960\n",
            "Epoch 30/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.77400\n",
            "100/100 - 16s - loss: 0.4792 - acc: 0.7680 - val_loss: 0.8868 - val_acc: 0.6350\n",
            "Epoch 31/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00031: val_acc improved from 0.77400 to 0.78800, saving model to /content/drive/My Drive/Colab Notebooks/weights.31-0.79.hdf5\n",
            "100/100 - 16s - loss: 0.4777 - acc: 0.7765 - val_loss: 0.4374 - val_acc: 0.7880\n",
            "Epoch 32/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4582 - acc: 0.7850 - val_loss: 0.5005 - val_acc: 0.7610\n",
            "Epoch 33/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4762 - acc: 0.7780 - val_loss: 0.5838 - val_acc: 0.7220\n",
            "Epoch 34/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4588 - acc: 0.7805 - val_loss: 0.5019 - val_acc: 0.7640\n",
            "Epoch 35/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4409 - acc: 0.7935 - val_loss: 0.6949 - val_acc: 0.7110\n",
            "Epoch 36/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00036: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4303 - acc: 0.7960 - val_loss: 0.4608 - val_acc: 0.7770\n",
            "Epoch 37/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4494 - acc: 0.7925 - val_loss: 0.5963 - val_acc: 0.7240\n",
            "Epoch 38/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4141 - acc: 0.8190 - val_loss: 0.6326 - val_acc: 0.7360\n",
            "Epoch 39/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4194 - acc: 0.8050 - val_loss: 0.4543 - val_acc: 0.7850\n",
            "Epoch 40/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.78800\n",
            "100/100 - 16s - loss: 0.4115 - acc: 0.8245 - val_loss: 0.5901 - val_acc: 0.6890\n",
            "Epoch 41/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00041: val_acc improved from 0.78800 to 0.80100, saving model to /content/drive/My Drive/Colab Notebooks/weights.41-0.80.hdf5\n",
            "100/100 - 16s - loss: 0.4288 - acc: 0.7985 - val_loss: 0.4802 - val_acc: 0.8010\n",
            "Epoch 42/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00042: val_acc improved from 0.80100 to 0.81300, saving model to /content/drive/My Drive/Colab Notebooks/weights.42-0.81.hdf5\n",
            "100/100 - 16s - loss: 0.4135 - acc: 0.8070 - val_loss: 0.4276 - val_acc: 0.8130\n",
            "Epoch 43/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00043: val_acc improved from 0.81300 to 0.83400, saving model to /content/drive/My Drive/Colab Notebooks/weights.43-0.83.hdf5\n",
            "100/100 - 16s - loss: 0.4049 - acc: 0.8150 - val_loss: 0.3794 - val_acc: 0.8340\n",
            "Epoch 44/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.83400\n",
            "100/100 - 16s - loss: 0.3969 - acc: 0.8125 - val_loss: 0.4125 - val_acc: 0.8310\n",
            "Epoch 45/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.83400\n",
            "100/100 - 16s - loss: 0.4016 - acc: 0.8310 - val_loss: 0.4384 - val_acc: 0.8000\n",
            "Epoch 46/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.83400\n",
            "100/100 - 16s - loss: 0.3781 - acc: 0.8340 - val_loss: 0.6318 - val_acc: 0.7440\n",
            "Epoch 47/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.83400\n",
            "100/100 - 16s - loss: 0.3698 - acc: 0.8360 - val_loss: 0.4289 - val_acc: 0.8100\n",
            "Epoch 48/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00048: val_acc improved from 0.83400 to 0.84000, saving model to /content/drive/My Drive/Colab Notebooks/weights.48-0.84.hdf5\n",
            "100/100 - 16s - loss: 0.3786 - acc: 0.8345 - val_loss: 0.4153 - val_acc: 0.8400\n",
            "Epoch 49/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00049: val_acc improved from 0.84000 to 0.85300, saving model to /content/drive/My Drive/Colab Notebooks/weights.49-0.85.hdf5\n",
            "100/100 - 17s - loss: 0.3837 - acc: 0.8350 - val_loss: 0.3599 - val_acc: 0.8530\n",
            "Epoch 50/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3550 - acc: 0.8440 - val_loss: 0.8868 - val_acc: 0.7050\n",
            "Epoch 51/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3628 - acc: 0.8455 - val_loss: 0.5542 - val_acc: 0.7160\n",
            "Epoch 52/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3619 - acc: 0.8365 - val_loss: 0.4751 - val_acc: 0.7980\n",
            "Epoch 53/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3646 - acc: 0.8445 - val_loss: 0.6032 - val_acc: 0.7440\n",
            "Epoch 54/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00054: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3573 - acc: 0.8420 - val_loss: 0.8275 - val_acc: 0.6970\n",
            "Epoch 55/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3722 - acc: 0.8345 - val_loss: 0.4449 - val_acc: 0.8200\n",
            "Epoch 56/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3315 - acc: 0.8555 - val_loss: 0.4283 - val_acc: 0.8410\n",
            "Epoch 57/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3458 - acc: 0.8530 - val_loss: 0.5249 - val_acc: 0.7680\n",
            "Epoch 58/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3299 - acc: 0.8605 - val_loss: 0.4557 - val_acc: 0.7900\n",
            "Epoch 59/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3208 - acc: 0.8595 - val_loss: 0.5777 - val_acc: 0.7890\n",
            "Epoch 60/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3416 - acc: 0.8445 - val_loss: 1.6826 - val_acc: 0.5900\n",
            "Epoch 61/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.85300\n",
            "100/100 - 16s - loss: 0.3368 - acc: 0.8565 - val_loss: 1.1145 - val_acc: 0.7830\n",
            "Epoch 62/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00062: val_acc improved from 0.85300 to 0.87100, saving model to /content/drive/My Drive/Colab Notebooks/weights.62-0.87.hdf5\n",
            "100/100 - 16s - loss: 0.3222 - acc: 0.8570 - val_loss: 0.3689 - val_acc: 0.8710\n",
            "Epoch 63/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3345 - acc: 0.8630 - val_loss: 1.4983 - val_acc: 0.7040\n",
            "Epoch 64/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3264 - acc: 0.8565 - val_loss: 0.4344 - val_acc: 0.8640\n",
            "Epoch 65/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3254 - acc: 0.8635 - val_loss: 0.4261 - val_acc: 0.8200\n",
            "Epoch 66/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3211 - acc: 0.8680 - val_loss: 0.4241 - val_acc: 0.8200\n",
            "Epoch 67/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3149 - acc: 0.8685 - val_loss: 2.1142 - val_acc: 0.6810\n",
            "Epoch 68/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3205 - acc: 0.8600 - val_loss: 0.3931 - val_acc: 0.8410\n",
            "Epoch 69/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3068 - acc: 0.8715 - val_loss: 0.6361 - val_acc: 0.7820\n",
            "Epoch 70/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3104 - acc: 0.8685 - val_loss: 2.2090 - val_acc: 0.7910\n",
            "Epoch 71/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3331 - acc: 0.8590 - val_loss: 0.6252 - val_acc: 0.8310\n",
            "Epoch 72/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3357 - acc: 0.8575 - val_loss: 0.4973 - val_acc: 0.8410\n",
            "Epoch 73/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3343 - acc: 0.8555 - val_loss: 0.8387 - val_acc: 0.8160\n",
            "Epoch 74/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3147 - acc: 0.8675 - val_loss: 0.4625 - val_acc: 0.8210\n",
            "Epoch 75/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00075: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3003 - acc: 0.8710 - val_loss: 0.3923 - val_acc: 0.8630\n",
            "Epoch 76/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3114 - acc: 0.8685 - val_loss: 0.4583 - val_acc: 0.8450\n",
            "Epoch 77/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3299 - acc: 0.8630 - val_loss: 0.4769 - val_acc: 0.8150\n",
            "Epoch 78/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3089 - acc: 0.8610 - val_loss: 0.4464 - val_acc: 0.8380\n",
            "Epoch 79/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3148 - acc: 0.8590 - val_loss: 0.4330 - val_acc: 0.8290\n",
            "Epoch 80/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3144 - acc: 0.8700 - val_loss: 0.4635 - val_acc: 0.8480\n",
            "Epoch 81/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00081: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.2952 - acc: 0.8710 - val_loss: 0.3742 - val_acc: 0.8450\n",
            "Epoch 82/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.2971 - acc: 0.8710 - val_loss: 0.5423 - val_acc: 0.8160\n",
            "Epoch 83/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.2881 - acc: 0.8815 - val_loss: 0.5922 - val_acc: 0.8250\n",
            "Epoch 84/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3189 - acc: 0.8755 - val_loss: 0.5636 - val_acc: 0.8300\n",
            "Epoch 85/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3090 - acc: 0.8820 - val_loss: 0.4938 - val_acc: 0.8140\n",
            "Epoch 86/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.87100\n",
            "100/100 - 16s - loss: 0.3096 - acc: 0.8770 - val_loss: 0.4465 - val_acc: 0.8630\n",
            "Epoch 87/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00087: val_acc improved from 0.87100 to 0.88700, saving model to /content/drive/My Drive/Colab Notebooks/weights.87-0.89.hdf5\n",
            "100/100 - 16s - loss: 0.2806 - acc: 0.8865 - val_loss: 0.4047 - val_acc: 0.8870\n",
            "Epoch 88/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.3122 - acc: 0.8900 - val_loss: 0.3771 - val_acc: 0.8710\n",
            "Epoch 89/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2699 - acc: 0.8905 - val_loss: 0.5497 - val_acc: 0.8750\n",
            "Epoch 90/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.3163 - acc: 0.8870 - val_loss: 1.5211 - val_acc: 0.6400\n",
            "Epoch 91/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2666 - acc: 0.9020 - val_loss: 1.1824 - val_acc: 0.8010\n",
            "Epoch 92/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2861 - acc: 0.8890 - val_loss: 0.3528 - val_acc: 0.8750\n",
            "Epoch 93/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2924 - acc: 0.8810 - val_loss: 0.6734 - val_acc: 0.8410\n",
            "Epoch 94/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.3083 - acc: 0.8780 - val_loss: 0.5682 - val_acc: 0.8180\n",
            "Epoch 95/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2715 - acc: 0.8890 - val_loss: 0.5436 - val_acc: 0.8370\n",
            "Epoch 96/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.88700\n",
            "100/100 - 18s - loss: 0.2989 - acc: 0.8710 - val_loss: 0.4716 - val_acc: 0.8290\n",
            "Epoch 97/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.2698 - acc: 0.8955 - val_loss: 1.0065 - val_acc: 0.7720\n",
            "Epoch 98/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.3270 - acc: 0.8795 - val_loss: 0.5116 - val_acc: 0.8240\n",
            "Epoch 99/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.3024 - acc: 0.8925 - val_loss: 0.5617 - val_acc: 0.8350\n",
            "Epoch 100/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.2919 - acc: 0.8790 - val_loss: 0.3913 - val_acc: 0.8560\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLI_32CawKP6",
        "colab_type": "code",
        "outputId": "16360ab4-8054-4534-c1c5-238c32413061",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO2dd7wU1d3/31/wUi4g3UITbPQiErBG\niVGxEhNjRGx5NFhSjL88PiGaRB+jeXwSo8bYYowaI7aoWGILUZ5gidICKCBCkKsUqQpIES73+/vj\n7GHn7p3dnW23zH7fr9e+dmfm7MyZnZ3PfOZ7vueMqCqGYRhGfGnW0BUwDMMwSosJvWEYRswxoTcM\nw4g5JvSGYRgxx4TeMAwj5pjQG4ZhxBwTeiNnRKS5iHwuIr2KWbYhEZEDRaToucYi8lURWRaYXiQi\nR0cpm8e27hORq/P9fob13iAiDxZ7vUb9sUdDV8AoPSLyeWCyEvgC2JWYvkRVJ+WyPlXdBbQtdtly\nQFX7FmM9InIxcK6qHhtY98XFWLcRP0zoywBV3S20Ccd4sar+PV15EdlDVavro26GYZQeC90Y/tb8\ncRF5VEQ2A+eKyOEi8raIfCYiq0TkdhGpSJTfQ0RURHonph9OLH9JRDaLyD9FpE+uZRPLTxKRD0Rk\no4j8TkTeFJEL09Q7Sh0vEZElIvKpiNwe+G5zEblVRNaLyFJgTIbf5xoReSxl3p0ickvi88UisjCx\nP/9OuO1061ouIscmPleKyJ8TdZsPHJpS9qcisjSx3vkicnpi/mDgDuDoRFhsXeC3vS7w/UsT+75e\nRJ4RkX2j/DbZEJEzEvX5TEReE5G+gWVXi8hKEdkkIu8H9vUwEZmdmL9aRH4ddXtGEVBVe5XRC1gG\nfDVl3g3ADuA03MW/NfAlYBTurm9/4APge4nyewAK9E5MPwysA0YAFcDjwMN5lN0L2AyMTSz7f8BO\n4MI0+xKljs8C7YHewAa/78D3gPlAD6AzMM2dDqHb2R/4HGgTWPcaYERi+rREGQG+AmwDhiSWfRVY\nFljXcuDYxOebgf8DOgL7AQtSyp4F7Js4Juck6rB3YtnFwP+l1PNh4LrE5xMSdRwGtALuAl6L8tuE\n7P8NwIOJz/0T9fhK4hhdDSxKfB4IVAH7JMr2AfZPfJ4BjEt8bgeMauhzoZxe5ugNzxuq+ryq1qjq\nNlWdoarvqGq1qi4F7gWOyfD9J1V1pqruBCbhBCbXsqcCc1T12cSyW3EXhVAi1vF/VHWjqi7Diarf\n1lnAraq6XFXXAzdl2M5S4D3cBQjgeOBTVZ2ZWP68qi5Vx2vAq0Bog2sKZwE3qOqnqlqFc+nB7T6h\nqqsSx+QR3EV6RIT1AowH7lPVOaq6HZgIHCMiPQJl0v02mTgbeE5VX0sco5twF4tRQDXuojIwEf77\nMPHbgbtgHyQinVV1s6q+E3E/jCJgQm94Pg5OiEg/EXlBRD4RkU3A9UCXDN//JPB5K5kbYNOV7Ras\nh6oqzgGHErGOkbaFc6KZeAQYl/h8TmLa1+NUEXlHRDaIyGc4N53pt/Lsm6kOInKhiMxNhEg+A/pF\nXC+4/du9PlXdBHwKdA+UyeWYpVtvDe4YdVfVRcCPcMdhTSIUuE+i6LeBAcAiEZkuIidH3A+jCJjQ\nG57U1MLf41zsgaq6J/BzXGiilKzChVIAEBGhtjClUkgdVwE9A9PZ0j+fAL4qIt1xzv6RRB1bA08C\n/4MLq3QA/haxHp+kq4OI7A/cDVwGdE6s9/3AerOlgq7EhYP8+trhQkQrItQrl/U2wx2zFQCq+rCq\nHokL2zTH/S6o6iJVPRsXnvsN8JSItCqwLkZETOiNdLQDNgJbRKQ/cEk9bPOvwHAROU1E9gCuALqW\nqI5PAD8Uke4i0hn4cabCqvoJ8AbwILBIVRcnFrUEWgBrgV0icipwXA51uFpEOojrZ/C9wLK2ODFf\ni7vmfQfn6D2rgR6+8TmER4GLRGSIiLTECe7rqpr2DimHOp8uIscmtn0Vrl3lHRHpLyKjE9vblnjV\n4HbgPBHpkrgD2JjYt5oC62JExITeSMePgAtwJ/HvcY2mJUVVVwPfAm4B1gMHAP/C5f0Xu45342Lp\n7+IaCp+M8J1HcI2ru8M2qvoZcCUwGdegeSbughWFa3F3FsuAl4CHAuudB/wOmJ4o0xcIxrWnAIuB\n1SISDMH477+MC6FMTny/Fy5uXxCqOh/3m9+NuwiNAU5PxOtbAr/Ctat8gruDuCbx1ZOBheKyum4G\nvqWqOwqtjxENcWFQw2h8iEhzXKjgTFV9vaHrYxhNFXP0RqNCRMYkQhktgZ/hsjWmN3C1DKNJY0Jv\nNDaOApbiwgInAmeoarrQjWEYEbDQjWEYRswxR28YhhFzGuWgZl26dNHevXs3dDUMwzCaDLNmzVqn\nqqHpyI1S6Hv37s3MmTMbuhqGYRhNBhFJ27vbQjeGYRgxx4TeMAwj5pjQG4ZhxBwTesMwjJhjQm8Y\nhhFzsgq9iPQUkakisiDx+LArQsqMF5F5IvKuiLwlIkMDy5Yl5s8REUulMQzDqGeipFdWAz9S1dmJ\nMa1nicgUVV0QKPMhcIyqfioiJ+Ge9DMqsHy0qqZ9UpBhGIZROrI6+sSjzGYnPm8GFpLyMAhVfUtV\nP01Mvk3g4RFG/bFxIzz2WPZyhmGUFznF6EWkN3AItcfFTuUi3NjaHgX+JiKzRGRCrhU0ovPkkzBu\nHKxa1dA1MQyjMRG5Z6yItAWeAn6YeP5kWJnROKE/KjD7KFVdISJ7AVNE5H1VnRby3QnABIBevbI9\n1c0IY9s29759e8PWwzCMxkUkR594ZNhTwCRVfTpNmSHAfcBYVV3v56uqf5bkGtzTbkaGfV9V71XV\nEao6omvXTE+PM9JRXV373TAMA6Jl3QjwR2Chqt6Spkwv4GngPFX9IDC/TaIBFxFpA5yAe5izUQJ2\n7qz9bhiFsH077LCH/cWCKKGbI4HzgHdFZE5i3tUknlivqvcAPwc6A3e56wLVqjoC2BuYnJi3B/BI\n4lmWRgkwR28Uk9NOg7594Y47GromRqFkFXpVfQOQLGUuBi4Omb8UGFr3G0YpMEdvFJOPPoLWrRu6\nFkYxsJ6xMcIcvVFMdu400xAXTOhjhDl6o5iY0McHE/oYYY7eKCYm9PHBhD5GmKM3iokJfXwwoY8R\n3snbyWkUAxP6+GBCHyP8SWmhG6MYmNDHBxP6GGGO3igmJvTxwYQ+RpijN6KgCj/5CSxcmLnMrl0m\n9HEh8qBmRuPHHL0RhU2b4KaboGNH6N8/vIw17McLc/Qxwhy9EYUoIm5CHy9M6GOEOXojClH6W5jQ\nxwsT+hhhjt6Igjn68sOEPkbYyWlEIYohsP9SvDChjxEWujGiYI6+/DChjxEWujGiEMUQmNDHCxP6\nGGGO3ohCLqGbmhr3Mpo2UR4l2FNEporIAhGZLyJXhJQREbldRJaIyDwRGR5YdoGILE68Lij2DhhJ\nzNEbUcgldJOtnNE0iNJhqhr4karOTjz/dZaITFHVBYEyJwEHJV6jgLuBUSLSCbgWGAFo4rvPqeqn\nRd0LAzBHb0Qjl/RK/7lly9LWySgtWR29qq5S1dmJz5uBhUD3lGJjgYfU8TbQQUT2BU4EpqjqhoS4\nTwHGFHUPjN2YozeiYI6+/MgpRi8ivYFDgHdSFnUHPg5ML0/MSzc/bN0TRGSmiMxcu3ZtLtUyEpij\nN6JgQl9+RBZ6EWkLPAX8UFU3Fbsiqnqvqo5Q1RFdu3Yt9urLAnP0RhTyCd0YTZtIQi8iFTiRn6Sq\nT4cUWQH0DEz3SMxLN98oAebojSiYoy8/omTdCPBHYKGq3pKm2HPA+Ynsm8OAjaq6CngFOEFEOopI\nR+CExDyjBFjusxGFXNIrs5UzmgZRsm6OBM4D3hWROYl5VwO9AFT1HuBF4GRgCbAV+HZi2QYR+QUw\nI/G961V1Q/GqbwSxh4MbUcilw1S2ckbTIKvQq+obgGQpo8B30yy7H7g/r9oZOWGO3ohCro7e/k9N\nH+sZGyPM0RtRsBh9+WFCHyPM0RtRsNBN+WFCHyMsvdKIgoVuyg8T+pgQHHzKTkwjExa6KT9M6GNC\n0J2ZozcyYR2myg8T+pgQPGntxDQyEcXR79hRt7zRdDGhjwnmwIyoWOim/DChjwkWujGiYqGb8sOE\nPibYiWlExRx9+WFCHxPM0RtRsfTK8sOEPibYiWlExRx9+WFCHxPM0RtRsZ6x5YcJfUzwJ2OrVnZi\nGpmJGrqpqKhd3mi6mNDHBH/Stm5tjt7ITFDoVdOXqaysXd5oupjQxwR/MrZubSemkZmgEdi1K7yM\nCX28MKGPCUFHbyemkYko8XcT+ngR5VGC94vIGhF5L83yq0RkTuL1nojsEpFOiWXLROTdxLKZxa68\nkSTo6C10Y2QiqtC3bAkiJvRxIIqjfxAYk26hqv5aVYep6jDgJ8A/Uh4XODqxfERhVTUyYY7eiEqU\nDC3fGFtRYf+nOJBV6FV1GhD1Oa/jgEcLqpGRF+bojahEdfQm9PGhaDF6EanEOf+nArMV+JuIzBKR\nCVm+P0FEZorIzLVr1xarWmVDqtCny6YwjKBwm6MvD4rZGHsa8GZK2OYoVR0OnAR8V0S+nO7Lqnqv\nqo5Q1RFdu3YtYrXKg2DoBtJnUxhGlCGtTejjRTGF/mxSwjaquiLxvgaYDIws4vaMAEFHH5w2jFQs\ndFN+FEXoRaQ9cAzwbGBeGxFp5z8DJwChmTtG4aQ6eovTG+mw0E35sUe2AiLyKHAs0EVElgPXAhUA\nqnpPotgZwN9UdUvgq3sDk0XEb+cRVX25eFU3gpijN6JioZvyI6vQq+q4CGUexKVhBuctBYbmWzEj\nN/zJa51cjGyYoy8/rGdsTEh19Ba6iQevvgrXXlvcde7cmf3Oz4Q+XpjQx4TUGL2dnPHgqafg5puL\nu87q6uyGwIQ+XpjQxwRz9PHkiy9g69biHk9z9OWHCX1MMEcfT774wr1v3ly8dZrQlx8m9DHBHH08\n8UK/aVPx1mmhm/LDhD4mmKOPJ6UQ+ihDEJvQxwsT+phgjj6ebN/u3ost9OboywsT+phQXQ3Nm9tz\nPuNGqRy9xejLCxP6mOBPzD32SE4bTZ9Sx+jD/ieqblA8E/r4YEIfE3budCLvHb2FbuJBqWP0Yf8T\nL+wm9PHBhD4mVFcnT0ywkzMuFFvod+1yjj1TY6wJffwwoY8J3tH70I05+nhQ7Dz6KKOcmtDHDxP6\nmGCOPp4UO+smyiinfl6LFib0ccGEPiaYo48nxQ7d5CL05ujjgwl9TDBHH0+KLfQWuilPTOhjgjn6\neFIqR5+pMXbHDvduQh8fsgq9iNwvImtEJPQxgCJyrIhsFJE5idfPA8vGiMgiEVkiIhOLWXGjNubo\n40dNTfI4lip0E8XRV1e7TJ1yYPVquPLK5MUuLkRx9A8CY7KUeV1VhyVe1wOISHPgTuAkYAAwTkQG\nFFJZIz2pefQm9E2foNgUO3RTUeF6UkeJ0Qe/F3defRVuuw3mzm3omhSXrEKvqtOADXmseySwRFWX\nquoO4DFgbB7rMSLgHb2FbuKDz7iBzEK/Ywe8HPFpzF7EvSmIKvTlYhy2bXPvq1c3bD2KTbFi9IeL\nyFwReUlEBibmdQc+DpRZnpgXiohMEJGZIjJz7dq1RapW+WCOPn74+HxFRWahf/55OOkk+Pe/s68z\nKOJ77BEtdBOcF3f8xdWEvi6zgf1UdSjwO+CZfFaiqveq6ghVHdG1a9ciVKu8MEcfP7zQd+nihD5d\nnNxfBKKEd4KhG3P0dfFC/8knDVuPYlOw0KvqJlX9PPH5RaBCRLoAK4CegaI9EvOMEhAcbdBPG00b\nL/RduzqR37Ilczn/nonU0I05+tqYo0+DiOwjIpL4PDKxzvXADOAgEekjIi2As4HnCt2eEU51taVX\nxg0v3Hvt5d7TOfZ8hN7f/Zmjr01chX6PbAVE5FHgWKCLiCwHrgUqAFT1HuBM4DIRqQa2AWerqgLV\nIvI94BWgOXC/qs4vyV4Yux19s2buVS4nZpwJOnpIP96NLxdsvE2HhW4yE9fQTVahV9VxWZbfAdyR\nZtmLwIv5Vc3IBd8YC+kb2YymRarQF9PR+7s/C93UJq6O3nrGxgTfGAvWmzEueNGJKvRRHH2U4Q3K\nWegtvdJo1AQdvQl9PCilow/2es1WJjgv7viL5WefRbtwNhWyhm4aCzt37mT58uVsj9OvX0Tuvhta\ntoSFC+Hxx91YJgsXNnStoFWrVvTo0YMKrxhGZHIV+lxi9D50Y46+NsHfcM0a6NWr4epSTJqM0C9f\nvpx27drRu3dvEkk+RoAdO6B9e+jdu/bnhkRVWb9+PcuXL6dPnz4NW5kmSKmzbszR1yUo9KtXx0fo\nm0zoZvv27XTu3NlEPgP+pxFpHINQiQidO3e2u7A8KXXoxhx9XbZvdw9cgXhl3jQZoQdM5DOg2viE\nHuyYFYIX7rZtoVWr4odurDG2Ltu3w377uc9xapBtUkLfUKxfv55hw4YxbNgw9tlnH7p37757ekfE\n8Uy//e1vs2jRooxl7rzzTiZNmpRXHVOF/qyzjmLOnDl5rctoHHjhbtkS2rWz0E19EFehbzIx+lyZ\nNAmuuQY++sjF2W68EcaPz29dnTt33i2a1113HW3btuU///M/a5VRVVSVZs3Cr50PPPBA1u1897vf\nza+C1HbwZqLjgRfuli1hzz2Ln15poZu6bNsGffq4Ni4L3TRyJk2CCROgqsoJYFWVm87TLKdlyZIl\nDBgwgPHjxzNw4EBWrVrFhAkTGDFiBAMHDuT666/fXfaoo5zDrq6upkOHDkycOJGhQ4dy+OGHs2bN\nGgB++tOfctttt+0uP3HiREaOHEnfvn156623ANiyZQvf+MY3GDBgAGeeeSYjRoxgzpw5tRx9OrZt\n28YFF1zA4MGDGT58ONOmTQPg3Xff5Utf+hLDhg1jyJAhLF26lM2bN3PSSScxdOhQBg0axJNPPlnc\nH8/ISq5CH8XRp4ZuzNHXZvt2FybbZ594OfpYCv0118DWrbXnbd3q5heb999/nyuvvJIFCxbQvXt3\nbrrpJmbOnMncuXOZMmUKCxYsqPOdjRs3cswxxzB37lwOP/xw7r///tB1qyrTp0/n17/+9e6Lxu9+\n9zv22WcfFixYwM9+9jP+9a9/oRotRn/77bfTsmVL3n33Xf785z9z3nnnsWPHDu666y7+8z//kzlz\n5jBjxgy6devGiy++SO/evZk7dy7vvfcexx9/fNF+MyMawWGK69vR+6E0ylXo997bhL7R89FHuc0v\nhAMOOIARI0bsnn700UcZPnw4w4cPZ+HChaFC37p1a0466SQADj30UJYtWxa67q9//et1yrzxxhuc\nffbZAAwdOpSBAwfuLh8U+jDeeOMNzj33XAAGDhxIt27dWLJkCUcccQQ33HADv/rVr/j4449p1aoV\nQ4YM4eWXX2bixIm8+eabtG/fPvJvYhSHL75woiPihD7bWDf5xOjTCX2wl3Xwe3En6OgtdNPISZf7\nWoqc2DZt2uz+vHjxYn7729/y2muvMW/ePMaMGROaWtjC528BzZs3pzrNwDQtW7bMWgaS7j2b0Kfj\nvPPOY/LkybRs2ZIxY8Ywbdo0+vfvz8yZMxk4cCATJ07kl7/8ZW4rNQrmiy9c2AbqP3RT7kJvjr4J\ncOONyafceyor3fxSsmnTJtq1a8eee+7JqlWreOWVV4q+jSOPPJInnngCcLH1BQsWRBb6o48+endW\nz8KFC1m1ahUHHnggS5cu5cADD+SKK67g1FNPZd68eaxYsYK2bdty3nnn8aMf/YjZs2cXfV+MzGzf\nnpvQ5xK6ydYzthyFXrW20G/cGJ9hEGKZdeOza4qVdROV4cOHM2DAAPr168d+++3HkUceWfRtfP/7\n3+f8889nwIABu1977tmejRvrCv2JJ564e+iBo48+mvvvv59LLrmEwYMHU1FRwUMPPUSLFi145JFH\nePTRR6moqKBbt25cd911vPXWW0ycOJFmzZrRokUL7rnnnqLvi5GZUjh6PyaSiDn6VKqroaYmGboB\n5+p9umWTxqcFNqbXoYceqqksWLCgzrxyZOfOnbpt2zZVVf3ggw+0d+/eunXrTp0xQ3X1ak3MV50/\nvwErmYIdu/wYN071wAPd5xtvdE3u27fXLbfvvm7Z6NHZ13nVVaqtWrnPl16q2rVr3TL/8R+q3bu7\nzxs3unXffHN++9CU8Pv6m9+oPv+8+/zOOw1dq+gAMzWNpsbS0ceZzz//nOOOO47q6mpUld///vc0\nb+4OY2PsGWvkT6qjB9cg6+cFywXfM5Hq1i10k8SHaXzoBuITp4/yhKn7gVOBNao6KGT5eODHgACb\ngctUdW5i2bLEvF1AtaqOSP2+kRsdOnRg1qxZteb5E9yEPl74rBtICv2mTe5h4UG8QEWN0XvxzvTg\nkXIXeh+6iUvmTZTG2AeBMRmWfwgco6qDgV8A96YsH62qw0zkS0dYY6wJfdMnzNGHxelzzbrJ9tyC\noNA3b56cF3eCQu9HDC0bR6+q00Skd4blbwUm3wZ6FF4tIxdSRd2EPh6kZt1AXaHftcu9IL/QTTZH\n7xtty03oW7aEDh3iI/TFTq+8CHgpMK3A30RklohMyPRFEZkgIjNFZObatWuLXK3ywBx9vIji6IPi\nnk/oZteuuv+VYBkoT6GHeHWaKlpjrIiMxgn9UYHZR6nqChHZC5giIu+r6rSw76vqvSTCPiNGjDCZ\nyoFCO0wZjZOg0Ldr594zCX3U0E1Y/D3Qh8+EPiH0ceo0VRRHLyJDgPuAsaq63s9X1RWJ9zXAZGBk\nMbbXEIwePbpOB6jbbruNyy67LOP32rZtC8DKlSs588wzQ8sce+yxzJw5M+N6brvtNrYGBvA5+eST\n+eyzz4DCYvTXXXcdN998c7TCRr2Si6Nv2TK6o/cxev+eGr4pV6H3DwY3oQ9BRHoBTwPnqeoHgflt\nRKSd/wycALxX6PYainHjxvHYY4/VmvfYY48xbty4SN/v1q1bQSNApgr9iy++SIcOHYC6Qh+cZzRd\nwrJuUse78UK/5575xej9vHRlfLlyEHp/oWzd2r3HKXSTVehF5FHgn0BfEVkuIheJyKUicmmiyM+B\nzsBdIjJHRLw13Rt4Q0TmAtOBF1T15RLsQ71w5pln8sILL+x+0MiyZctYuXIlRx999O7c9uHDhzN4\n8GCeffbZOt9ftmwZgwa57NRt27Zx9tln079/f8444wy2eSsBXHbZZbuHOb722msBN+rkypUrGT16\nNKNHjwagd+/erFu3DoDf/e4WvvWtQRx++CBuu+02RGDFimX079+f73znOwwcOJATTjih1nayccst\ntzBo0CAGDRq0e+jkLVu2cMopp+weuvjxxx8HYOLEiQwYMIAhQ4bUGaffyJ+go6+sdKNJpnP07du7\nZwXX1GReZ5jQm6N3hIVuNm2KxzAIUbJuMlpWVb0YuDhk/lJgaP5VS88PfwjFfnjSsGGQ0LNQOnXq\nxMiRI3nppZcYO3Ysjz32GGeddRYiQqtWrZg8eTJ77rkn69at47DDDuP0009P+xi9u+++m8rKShYu\nXMi8efMYPnz47mU33ngjnTp1YteuXRx33HHMmzePH/zgB9xyyy1MnTqVLilJ1LNmzeLhhx/gwQff\n4cADla9+dRT9+h2DakcWL17Mo48+yh/+8AfOOussnnrqqd2jV2Zi1qxZPPDAA7zzzjuoKqNGjeKY\nY45h6dKldOvWjRdeeAFwwy2vX7+eyZMn8/777yMiu8NJRuEEs278CJbphN47/h07kkIVRjC90r+b\no3eENcZCPIZBiOWgZqUiGL4Jhm1UlauvvpohQ4bw1a9+lRUrVrA6Q3Bv2rRpuwV3yJAhDBkyZPey\nJ554guHDh3PIIYcwf/780GGOg7zxxhuccsoZtG7dhnbt2vL1r3+d6dNfRxX69OnDsGHDgMzDIYet\n84wzzqBNmza0bevW+frrrzN48GCmTJnCj3/8Y15//XXat29P+/btadWqFRdddBFPP/00lamjyRl5\nE3T0kFno/SjS2cI3UUI3O3aY0EOyd2wcwjdNcgiETM67lIwdO5Yrr7yS2bNns3XrVg499FAAJk2a\nxNq1a5k1axYVFRX07t07dHjibHz44YfcfPPNzJgxg44dO3LhhRfmtJ7UrJuWAZVo3rx5TqGbMA4+\n+GBmz57Niy++yE9/+lOOO+44fv7znzN9+nReffVVnnzySe644w5ee+21grZjOHIReu/ot29Pin4Y\nqemVYKEbT6rQ+9/0888bpj7FxBx9DrRt25bRo0fzH//xH7UaYTdu3Mhee+1FRUUFU6dOpaqqKuN6\nvvzlL/PII48A8N577zFv3jzADXPcpk0b2rdvz+rVq3nppWSXhHbt2rE55MkTRx99NC+88Azbt29l\n69YtTJ48mVGjji5oP48++mieeeYZtm7dypYtbp1HH300K1eupLKyknPPPZerrrqK2bNn8/nnn7Nx\n40ZOPvlkbr31VubOnVvQtg2H7whVbEef2jMWLHTjSRV63yib+rS6pkiTdPQNybhx4zjjjDNqZeCM\nHz+e0047jcGDBzNixAj69euXcR2XXXYZ3/72t+nfvz/9+/fffWcwdOhQDjnkEPr160fPnj1rDXM8\nYcIExowZQ7du3Zg6deru+cOHD2fcuAu54IKRtGoF3/nOxQwZcgjTpy+LvE833HDD7gZXgOXLl3Ph\nhRcycqTLhr344os55JBDeOWVV7jqqqto1qwZFRUV3H333WzevJmxY8eyfft2VJVbbrkl8naN9HjB\nDsbb99wTNmwILxd09JmI2hgbzKsvF6H3N7zBBvDg/CZNumEtG/JlwxTnxrp1qjNmqCZGL9ZPPnHT\nO3c2bL08duxyZ8MGN0zubbcl5511lmq/frXLTZ7syl19tXufNy/zeocMUR071n1+6in3nTlzapfZ\nay/VSy5JTh91VLQhkJs6P/6xasuWyemlS93v88ADDValnCDDMMUWuokBYWPdhM03mg7emddH6MZi\n9A7/dClPnBy9CX0MSDcEggl90yXY49UTtTE2E2GNsRajd6QKfZxi9Cb0McKEPj6ECX27di4DJNgp\nqhTplSb0Di/05ujrGTXlCg7aNjkAACAASURBVCVsCITg/IbEjll+hAl9mzbuPSg8uTp6C92kJ1Xo\nKyrcyxx9PdKqVSvWr19vwhFCYw3dqCrr16+nVaaumkYoYVk3Xui3bKlbLh9HHxa6qalxLxN6R+vW\n8RD6JpNe2aNHD5YvX46NVV+XjRvhs89g0SI3HsrWrbBuHXzwQe00uYagVatW9Ohhz6LJlTBH7xsH\nw4S+WOmVXtDLUei3basr9JWV8QjdNBmhr6iooE+fPg1djUbJ//wPXH11cmyUZ56BM86A2bOhf/+G\nrp2RD5lCN0GH6cv58epzyboJc/TlLPRxdvRNJnRjpMefhNlir0bTISy9Ml3opkWLpEAV2hhb7kLv\nG2A9cXH0JvQxoLraxeX9g5zTpc0ZTYdcQjctWyaF3kI3+WOO3mjUBJ8aBOnT5oymQy6hm5Ytk+Uy\nOXpVC91kIkzozdEbjYbgc0Ah/aiERtMhV0fvy2Vy9Lt2uXdz9OGUvaMXkftFZI2IhD4KUBy3i8gS\nEZknIsMDyy4QkcWJ1wXFqriRxBx9/MiUXhnm6Js3d/+BTI4+VcTN0dfGHD08CIzJsPwk4KDEawJw\nN4CIdAKuBUbhHgx+rYh0zLeyRjipjt4aY5s+mUI3YY4enEhlcvTpGu2jCn1D98soNWHplcVy9NOn\nw8KFha8nXyIJvapOAzZkKDIWeCgxiNrbQAcR2Rc4EZiiqhtU9VNgCpkvGEYepDp6a4xt+oRl3WQK\n3fiymRy9v/DnGrrx/ycf+okrpXT0F18MP/lJ4evJl2LF6LsDHwemlyfmpZtfBxGZICIzRWSmdYrK\nDXP08SPM0YcNspWL0BcSukktFzd27XL7VypHv369ezUUjaYxVlXvVdURqjqia9euDV2dJoU5+vjx\nxReul3PwuDZr5oSn2KGbKI2xwWVxxF8gw/LoiyH0mza53usNRbGEfgXQMzDdIzEv3XyjiJijjx+p\nz4v1tGmTv6NPDd2Yo0+S+hhBT+vW7mHphYStdu1yo47GQeifA85PZN8cBmxU1VXAK8AJItIx0Qh7\nQmKeUUTM0cePL76oKzrghL5QR++F23ewM6FPL/TFePiIf4ZAQwp9pLFuRORR4Figi4gsx2XSVACo\n6j3Ai8DJwBJgK/DtxLINIvILYEZiVderaqZGXSMPwoaV9fONpkk6R19ZmX9jbGroRsT9Vyx0E03o\n27bNb90bN7r3zz+v3WGtPom0SVUdl2W5At9Ns+x+4P7cq2ZEJfXPYx2mmj5+gLpUMoVusjn61NAN\nuP+KOfqkYw8L3UBhcXov9P5z5875rytfGk1jrJE/5ujjRykdfep/xRx9/YRuoOHCNyb0MSDV0Vtj\nbNMnn8bYVq1yC934z+boMzfGQvEcvQm9kTepjt4aY5s+mYQ+k6PPNXRjjt7hf7ew9EoozNGb0BtF\nwRx9/EiXdVOK0I05enP0RhMg1dE3a+YyKkpxYr7yCvzzn8Vfb2Nmyxa49db6HQIgSuhGtbD0SrDQ\njcdi9EajJyxlK/WWvFhMmADXXVf89TZmXnwR/t//gxkzspctFumyboKO3gtvrh2mUu/+LHRjjt5o\nAqQ6eqjr1IrBunXw0Uewocx6QviT85NP6m+b2Ry9d/Ngjr4YpEuvLFaMvlMnd5fdUELfZB4ObqQn\ndQgEKM0Y4v/6l3v/9NPirrex4x3Z6tX1t81MQg9OeFKF3tIr86fUjr5jR6ipqe3u6xMT+hiQOgQC\nuOlih25mz3bv5ebo/cnZGBx9cKjiHTvc56Cjr6lJ3/syXejGHH32GH2hQr/nnq6Nx0I3RkYefhiu\nvDJ8WX05ei/0n33mBKVcaCihTzfWDTjhCXP0kD58U4rQzV//Ctdem34/mgrZHH2hjbHt20OHDib0\nRhYefxz+8Ifwp/yEOfpSNMZ6oVdtuFvQhiBXoVeFm24q7IlC2UI3W7aEx+j9d8OIGrpp1sy9gmWC\n3w/y8MNw222Z96UpsH27G+Qt1TA1bw4tWhTu6E3ojUhUVbmTO0xgwxx9sRtjN26EJUugb183XU7h\nm1yFft069zShP/85/21myrqB/Bx9WOgm9X+yY0f43SGE/59Wr3aONVPbQFMg7OlSntatC2+MNaE3\nIvHRR+7944/rLqsPR+8bYo87zr2XU4Nsro2x//63e1+3Lr/tpebHB8nk6P17oY4+V6GH/Pe1sZBJ\n6At9+IiP0ZvQGxnZuDEpNsuX111eH+mVPmzjhb5cHX2UB2R7oc/3iZjV1W472RpjixW6SY3R5yP0\nTf3pn2EPBvcU8txYVYvRGxGpqkp+DnP09dFhavZs6N4d+vVz0+Uo9Nu2webN2csvWeLe83W5Yc+L\n9RSjMTbboGZRhX7HjuT/YM2a8G02FbKFbvJ19Fu2uMQFL/SbNzfM0CQm9E0AH7aBhnX0hx7qOn5A\n+YVu9tzTfY4Spy/U0XsBz5R1k4+jjzqoWVShD4p7U3f02UI3+Tp6bxK80EPtIRHqi0hCLyJjRGSR\niCwRkYkhy28VkTmJ1wci8llg2a7AsueKWfl8mDMnGW9uKnhH36pVXaGvqXGvMEdfLKHfsgXefx+G\nD3cdP6B8HL3PMPKN0FHi9MUS+mI3xkZNr4wq9MHfIs5CX4ij90LvY/TQMOGbrB2mRKQ5cCdwPLAc\nmCEiz6nqAl9GVa8MlP8+cEhgFdtUdVjxqpw/y5fDV74CXbrABx+kL7d5MyxdCkOH1l/dMvHRR+4k\nHjy4bugmzKX56WLdIs6Z4wRv+HBXj8rK8nH027a53/Hgg91YN1EcvQ/dfPppfo+O80JdqvTK1NTJ\nfB193IQ+dYhiT2Vl/mG4oKP3HdwaQuijOPqRwBJVXaqqO4DHgLEZyo8DHi1G5YpJTQ2cf747+RYv\nhkWL0pe9+moYObJhbrHCqKqCnj2hV6+6jj4sZc5PF8vR+4bY4cPde6dO5ePo/YnqHX02od+82YU0\nund3F8d8fqdMjt6LUaasm0zplRUVbswVTyGNsV7oReIh9KVw9F5DgqGbxir03YGgj1yemFcHEdkP\n6AO8FpjdSkRmisjbIvK1dBsRkQmJcjPX5vGvmTQJevd2bqV3bzcd5De/galTk734/vrX8PXs2gV/\n+Yu7+r75Zs7VKAlVVbDfftCjh3P0wcyPsNtxP10sRz97Nuy1F3Tr5qY7diw/od9/f9d5JpvQL13q\n3g87zL3nI4CZhL55cydIYaGbKI4+W1tOWBkRt910Qt+nT7yFvpD0yrAYfWMV+lw4G3hSVYMjd++n\nqiOAc4DbROSAsC+q6r2qOkJVR3Tt2jWnjU6a5IbPrapyIlhV5aa92M+eDddcA9/4hhP6IUPSC/0b\nbyT/wFOn5lSNkvHRR07oe/as22mqvhz98OFJJ9ipU/mEbvxv3bEj7L13dqH38Xkv9Pnc8mcSekg+\nZSqfPPps2Vk7d7qeoKmEtfmsXg1t27r/ZlMX+kzplYV0mGosMfooQr8C6BmY7pGYF8bZpIRtVHVF\n4n0p8H/Ujt8XhWuuqXvF3brVzX//fTjjDOjaFX7/eydWp54Kr78eLlZ/+Ys7sIcemp/Q79oF//u/\nxetAsmMHrFrlwjY9erh5wfBNqR29qgt1DRiQnFeOoZv27Z3QZ2uM9fH5UaPce7EdPSTHpPflvDBH\nDd0EieLoIb3Q7723O7eautCbo4cZwEEi0kdEWuDEvE72jIj0AzoC/wzM6ygiLROfuwBHAgtSv1so\nwfTDIFVVMHCg+2Gffx46d3bzTz3VCfIrr9Quv2sXPPUUnHwynHKKc7K5juny5pswcSLcf3/u+xGG\nD9V4R+/neUrt6Fevdm6mT5/kvI4dy8/Rt28P++wTzdF36QIHJO5bCxH6dMLjx6QvRugmSmOsL5f6\nf/rkk/IQ+kIdvYi782nXruHGpM8q9KpaDXwPeAVYCDyhqvNF5HoROT1Q9GzgMdVafQf7AzNFZC4w\nFbgpmK1TLHr1Sr+spsY1iBx6aDJ2P3KkOxlTwzdvvun+vN/8Jhx7rPvu66/nVpe33nLvb7yR2/fS\n4S9iPkYP9evoP/zQvQeFPl9H/9ln8OyzhdepPslH6A84wP2/IL87u0xZN1Db0YskL/JR0iuzGYJ8\nHf2nnzbtYYyzOfqdO/M7nzZtcmEbP1Bc+/aNVOgBVPVFVT1YVQ9Q1RsT836uqs8FylynqhNTvveW\nqg5W1aGJ9z8Wt/qOG29M5hdnoqoKzjvP/bm3bYNnnql98P7yF3ewTzkFDj/cnTjpwjdbtsA999Q9\n+L4B9803izOUr8+h79UL9t3XndhBoS+1ow8T+o4d3e+X6WlGYfz+9/C1r8HKlYXXq75IFfrVqzMf\n1yVLnNC3aOG+U4rQTdDRt2yZbDuJ0mEq23DWuQr9Pvs4oQdYvz79PjV2sjl6yM/V+wHNPA01DEIs\nesaOHw/33utcbzb8/caWLe713//tpmtqXNjmpJPcbVarVk7s0wn9rbfCZZfBc4Eglqpz9O3bO8db\nyDC1nqoqdyL37OlOtn33rR26yeToiyn0vXsn5+XbO/a999y7b7BsCgRvvffe24lluruZHTvcsTnw\nQDfdpUvphN47+mCZPfZwdc3k6MP+J6rJi1dUod+50wm7d/TQdMM3qtnz6KF2nH7WrGgOP9irGkzo\nC2b8eFi2LJrYB7nhBhcS+fGPXaPnvvsm0zTfecf1ohWpnbK5bRvcfrv7/MILyXUtWuRE4PLL3XQx\nwjcffeTq5BvcevSI5uiLGbrZe+/ad0xe6HMN3/gLn09BbAr4E7VZM+deIX2D7LJlTjB9fL5r19Jk\n3QRDN8EyIs6g5JJ146e9iEcVei/qcRD6HTuc2Ed19CtXwpe+BHffnX3d5uhLRNQwTpAVK+Dmm93n\nu+5KpmkGb9WCKZt/+pP7Ux98sBN674Z8fP6885woFEPoq6pqt0H07BnN0aeGbvIdL/zDD2uHbSA5\nDEIujr6mxmVA+XU2FYInqhf6dHF6f6cSFPr6CN0EyfTc2HShG78Mogu9v9jFQejTPV3Kk+rovT5E\naW/yI1d6TOiLRGoYJ9gLsFC2boVzz4XvftcJ6QcfuD/8DTe45W++6dxu375w1FHFE/rgXUpqp6ko\nQyDMmeNa/OfMyX37YUKfj6Nfvty5UGh6jj6q0PvUSi/0hYZuMglPmKMHN51L6CZfR+9/g3ISem/8\nVq1y79OmZR/N1Bx9CfFhHFX3lJ9ii75/ALPnv//bndT33+/+DI8+6oR+2bLw0SZz2c7HH9cWet9p\nynetDht61k/7ZS+/7D6/+mpu26+udqGjdI4+F6H3YZvWrTMLvSrMnx9t3Pd82bEj2nDDkLujb9PG\niR8kQze57kshjj7X0E2hjn6ffVzaclMeBiGb0PvQjXf0/vjv3AlTpmRet8Xo64l0ol9MamqSGQfb\ntrkQjxeSQlz9mjXupA2GbnyKpQ/fREmv9JlA06fntv3ly13fgnSOPpfQjQ/bfOUrmYX+N7+BQYPc\nxalUTJyY7LmajaDQ77mnE9Z0MXqfWukNRdeuuV1UPNnSK9M1xvrv5NJhKjiOjaoToaAwBculC900\nb+7+E3EV+tTQzapVrs2mQ4fabXRhhDn6hhiTPvZCH8SL/sMP5x7Hz4WtW+FnP3MnfCEdp3xqZWro\nBpJ3CtnSK2tqkm0HM2bktn0vyKlCv+eebt9ydfSdOrkeo6tWhaeqTZ4M//Vf7rMfSK0UvP02LFgA\nn3+evWzwRBXJnEvvhd7jc+lzFcAvvnDHr1mas7OyMjl8cq6OPlPoZt06dwEJZlh5Ond2bVme1avd\nBcePptmUO01FdfTB0M1ee8GJJ8KLL6ZPt92+3V3og0LvP9f3gIllJfSebHF8P925c7I3bT6ouls7\nEXfSd+mSftC1MIKdpTypvWMzOXpVJ7AbNkD//i7ensvJGJZDD24fcu0du3Chq4MXwmXLai+fNcsd\nl5Ej3cXs3XejrzsX/G8CmYeq9qQ6snRCX1PjLoxBofex61wzb9I9L9bjxXXDhtxj9JlCN/6YpB5v\ngCOPdFll/mEjvrOUJ85Cn+roP/nEZcKdcor7nM6UBEeu9DTUMAhlKfQQHtIRce9//rObv26dexXj\nDmD9evcKG3QtHcHOUp7UTlOZHD3AP/7h3n/4Q/c+c2ayzLZtMHZs+pDOhx86Ue/Zs+6yXHvHeqH3\nIhIM36xZA6ed5lzSs8/CsGHJnPtis2ZN8iTLNFQ1JF1zFKFfutQJ9MEHJ+fl20iZTej9f/HTT4vr\n6NNd2AGOPtq9+1BkYxD6NWui3ZVlwzv1dHn0YY5+331hzBh3LqYL3wQHNPOY0DcgXvRratz7+PF1\nl/s7AJGk0/ef091iZ8Jn8ASdfqrr/9vfnMgEhcZ3mvJCn8nRA/zf/7l1nn22q29Q1F980XX4uu66\n8Dp++GGyo1YqmRz9ypXwz38mp9evdyLQv78b7hdqC/2zz7qT54knnHgMHuxE2D+oAVxIoVcvePzx\n8G1GJdiJLZvQb9/uft8oQu+HyjjiiOS8QoQ+nbuE7I4+3/RK7+jDQjcjRrg6TZvmphta6Nevd/+T\nYcMKT9fNJ0bvewSPGpVd6M3RNyGCFwPv9P1nH1fOh6DTT3X9U6a4P0tqqMenWELmDlPgHP0RRzhX\n0b9/7Tj9U0+595deSqYGBglLrfRkcvSXXw6jRycvBF5c+/d3rr2ysrbQv/WWuxh96UtuetAgt19B\nIX7nHbfPUUJemfB1adMmu9CHnah77+2OeWpj2uuvu4t+//7JefmOd7N9e7TQzY4d4Y4+n9CNd/Sd\nO7tU3FRatHA9xb3Qf/JJMgsJnOitX1+cYT+iMHFi8nw58sjC7gBzidHv2uXuJPbd18075RR3ToU1\n0JvQx4xf/tKd6A89VJpGXj9Gj4/1z5njLgJdusDFF7syxx9fWwT9Cb1mjTsRwMW/p09Pdvl+/nkX\nMqmogDvvrLvdTEKf7uEja9c6h/PFF/DYY25eUOhFnKsPurC33nIXI982MmiQew+evL5B+bXXajv9\nXFm40A1ncMQR+Qn9Pvu43y/VvU6b5lJqg3d3bds6Ic7V6b73Xrir9gT/Y7k4+iihm0zb/fKX3X/P\nC2yqo6+pqZ/hq994A+67D668Mnkn9eUv555V5sklvXLdOif2XuhPPdW933573TTaKEK/ZUv9/GYm\n9EVAxJ3k551Xus5a/k+0fn1S6NavT8YoV65MXgx6967dQBQU+rVrXSPv3/7mvvvd78KZZ7rsoGC8\nc9s259oyOfqw0M2jjzq3u88+8MADbt7ChU6cfFtDnz5JR792rWsU9XUE6NfPCVCq0Ddr5k6MQp78\ntXChW3+/fm67mXLc0wk91A7frFzpMm6+/OXa3xfJPaSxYQPMnevuiNLhHT0UL73Sh27SHW9w+6fq\nBgNUrSv0UPrwzc6dcOml7r903XXOFLz5pjtG55+f3x1FNqFv1sz9rtu2JTtL+f/B0KFutNtf/tKd\nf8HxcHxjbLoYfU2Ni/OfcELudc4VE/oik66Rt1OnzHHXYuBFq6oqKbKQzC7xoZHp013YpmNHl9f+\n/e+7P+Wf/5z8TqYMDEgKfeqJ9ac/wSGHuHDWjBmu89PCha63sHe7++/vhF41GcsPxrZbtHCNmj7z\npqbGlfvWt5wwFZJj7xuF+/Z1F41MI2mGCb3Pqgm6x6CrTKVLl9xCN//4h/tdjj02fZlMjj7fxtgd\nO9wxz+ToDzvMlf/LX9x0Qwj9Lbe4/9Tvfpe84PXp44Y+WbTIhSFzxffYznR++oeP+Au8d/Qi7s71\nhhvgkUecYfHtZ2H/H5+a/Nlnzly98YYbTyvXkWBzxYS+hATj+uvXO0dQ6hz+MC65pHYM/PzzXZhp\nxw7XAHrYYW68/jvuSF4sMmVggLtI1NTU7gz03nvuTuKCC1xD8x57wIMPJsXVs//+7uRau9Y59YoK\nt/0ggwYlHf3Che7EOPFEd+eUr9Bv2uRywb3QQ+bwTdiJOniwq9sfAwNuT5vmwjTDhtVdR66OfupU\nFyoYOTJ9mXwdfaYY/fLl7gKRydFXVrpGWd/Dur6FvqYGbrrJxcVPP732sm9+0z2Q/dZbc1/v4sVu\n3/baK30Z//AR7+i90IMzMNdc455vsWSJCylBeNZNs2ZuevFiZ4bat3f7NX9+7vXOBRP6eiZbBo/P\n2y9m2McPKevxQrBli7vdbNbM9VxdsCCZ8fPEE65MJkcPteOLf/qTE5JzznEn/qmnunlVVXWFHpyr\nf/NN9zza1NS2wYPdxebzz5Px+SOOcLe68+blN6a9F/VChF7EtYvMmOFCLOCE/ogj6oooZBb6v/2t\n7oNtpk51rjBKYyzk5ugzhW4WL3bvmRw9uLsW3xCd2hgLpRX6ZcvcBX/s2LrLKircnemrr7r/Ry7M\nm+cu3s2bpy/jHX1q6CbIySe7RITJk13iwMaN7nupv3mHDu4u4PPP3fnh61BKIgm9iIwRkUUiskRE\nJoYsv1BE1orInMTr4sCyC0RkceJ1QTEr31TJlMHjx0YpxRg9YQTH5/dUVbmG3YqK8D801B3vprra\n3a2cckrypL/wwuSJ369f8rte6BctcoIZjM97fIPs/PnJrJwDD3RCD3UfAxmFYKNw9+7uJMxV6MHd\nrbRo4Vz9hg3uziMsbAPpQze7drn/wdlnJ9tc1q5168oUn4fSNMb68F4mRw+19zPo6PPtBZwLflC+\noUPDl0+Y4H6bXFy9qhPZIUMyl/OO/pNP3P8hXc795Ze7dd59d92RKz0+Tn/VVc4MVVY2AqEXkebA\nncBJwABgnIgMCCn6uKoOS7zuS3y3E3AtMAoYCVwrIh2LVvsYky7WH7wD6NQpvxz+KFRXO2HYf//w\nlMbU8W7+/nd3ElwQuJSffHJS9IOO3rvGp55yohSMz3uCmTfBrJzBg91tcz7hm4ULndAdcID73Q4+\nOLvQi9RNN+zcGb7+dXdc/KBW6YS+a1d3wqeK7xtvuAvAypXJmLfv3JZN6LM5+h07whslM4VuvKPP\nNhbUkUe636Sy0oWrPIU8USsqc+e64+b/G6l07OjMxSOPZH/ko+eTT9zdbjahDzr6YNgmlf32c09R\nu/del3IZNm5Qt27ugnrNNe4uYvDg5N1hqYgiEyOBJaq6VFV3AI8BITdPoZwITFHVDar6KTAFGJNf\nVcuXdHcA69e7sMGll5buDiA1tdN36DrtNLf8+OOdcP/85078N21KPrjloINch5J27dxnT2Wlu1Pw\nDWdhQr///s41TZ3q3KYvI+Ji9VOm5D4w1MKFrh5e7Pr2zS707dqFX0wvvtiFEa6+2omcb/9IJd0w\nCE8/7UT6wANdA6Oq29c2bVwcPBPNmycFPszRQ3gKaljoxv8WixfXfcBMGB06OEcddPOerl2TQySU\ngrlz3fHLVMcrrnD7ftdd0dbpnfTgwZnLBYU+3V2u5/vfd+fmSy+FO/oHH3TJBX4/hgxx9SjliK1R\nhL47EHjUBcsT81L5hojME5EnRcR3mo/6XURkgojMFJGZa5vqoBkNwJFHutvEc89Nfwfgn06VL8HU\nTt+hy4c1wF0MZsxwLuXyy5MPZqiqcjHTX/yibh32398JT58+4Q6pWTMYOBCefDK5n54xY9ydRK6D\ntKU2Cvft636zdI2XqcMfBBk9OpkmOmpU+oyNsE5Tqi6Oe+KJ7vZ99mx3wZ461TU2h/VETsWLRJij\nh7r7pOrCReli9Bs3Zg/beK67zl3gUil179i5c9OHbTwHH+wc9a23JocQyURUoQ+GbjI5eoBjjnHr\nq64O///stVftC+WQIe68KuWzlIt14/880FtVh+Bc+59yXYGq3quqI1R1RFdvg4y8SL0DuP/+uqEf\nKH7sf9as2nnE4E6OH/6wbu9eH6cPc/OewYNdyCM1K+f4492FwHfIisKOHS7XPVXoVcN7BUNmoW/W\nDC66yH1OF7aB8EbKWbNcY93Xv+7ulrp0gZ/8xF2IsoVtPD58k87Rp4aK0j23ICj82RpiPWPHJjvq\nBckm9KouXPeNb6T/zdOxcaP7T4dlNqVy661uW5dckt0lz5vn2muyDV4YNXQD7rz6wQ/c53T/nyD+\n4lXKOH0UoV8BBIe16pGYtxtVXa+q/q91H3Bo1O8apScs9JMu9l8qUgdy80K/xx7JUE/qxcDHYlOz\ncjp1gm9/292e+3Hus7F4sXO0QaH3DcTpwjeZhB6c0I8Y4TqcpSMsdPP00y78ctppbr8uvzzZnyCq\n0Kdz9OmEPt2TyILCH9XRpyOT0H/wgbsTO/NMt//HHptsF0hF1fWuDpoGL4LZHD24/9FNN7kG+4ce\nylw2SkMsuOO0erWrU7bQDSQzz7qHxi9q4+8mGlroZwAHiUgfEWkBnA08FywgIsFr3OmAHzrqFeAE\nEemYaIQ9ITHPaASEXQBKmefvB3Lr3TuZ7vn447VDPcGLgRf6MNf/y186V3vFFdFim8GMG48faTJf\nod9nHxc+yuQyUx29d7WjRycbtC+/3Al0u3buohaFdI4+Xegm2+B3EN3Rp6NnTxejTw1BvPaaO5Zv\nvw2//a0LVX3xhRP71N++pga+9z2XjeIf0QnJxsooQg/uNz3qKHc36VMiU9m50/0vogh9ZWUywyyb\no/fl58ypvQ/p6NDB9fQtZYNsVqFX1WrgeziBXgg8oarzReR6EfHdFn4gIvNFZC7wA+DCxHc3AL/A\nXSxmANcn5hmNlFI+c9dTVZUcWydVkLZuddkI4Bo4DzrIhTiCTJrkOhRt3Ohy0X/0o+zb9M7f58+D\nyxrp3j1/oY9Cx47uN1ywIDkW/gcf1N6nvfeGa691ohSWix9GsUI3xXT0557r9jE4bpIq/PSnLtNk\n0SIX0jjkENceUV3txP5Pf3L/g5oaJ9B33eV+98ceS17E5851d5zdukWrS7NmbjycbdvgssvCzcCi\nRe53ieroPVGEHlxdg5lJmRg6tMQplqra6F6HHnqoGo2Dhx9W3W8/VRHVzp3dy3/eYw9VdwoV/xW2\nvc6dk8vCvrPffq6+qwvPMQAADE5JREFUqWzerHrYYW55Kl/5iuqoUeH73rWr6iWXFP4bHn+8q99h\nh6mec46r/8qVha3zhBPcOl96qfb8v/7Vzb/rLrffnlWrkvODfPpp8vf74IPC6qSq+rWvqXbqpLpl\ni5ueNs2t+4476padP1910CC3vGtXdyxA9Sc/UX3gAff57bdd2S99yS3PlZtvDt9vVdVJk9yyefOy\nr+fqq5O/03vv5V6PbFxzjWrz5qrbtuW/DmCmptHUBhf1sJcJfdPgmWdUL71UtbKytuBWVqpedlnd\n+fXx8hcBL/pVVaq9eqW/GFx+uWrLlqrXXae6enXt/WvRQvW//qvw32nnTtX77lPt1s3V4YgjCl/n\nGWe4db32Wu3577+v2ratW1ZRoXrkkapnn6163nlu3h/+ULv8558nf7ft2wuv1z/+4dZ3zz1u+uST\nnYh74U+lpkb1739XPe00V4ef/czN++wz9/tfcYX7/Vq1Ur3yytzrs2uX6kknuWM8Z07tZT/+sfuN\nvvgi+3p+8Yvkf2j9+tzrkY3HH3frnj07/3WY0BslJej6g0Lq59e32Gd7VVYm67hsmRMjcGLwve85\ncdi2zc278cbi/U5btqjeeafq9OmFr2v8eFe/N9+su2zbNtUpU9xF6ogjVA86SLVLF3cB+Mc/apfd\nvt2tp3v3wuuk6kT6kENU+/VzwgqqN9wQ7bupF4OvfU11332dgwbVBx/Mr05r1rj19O1b+y7npJNU\nhwyJto7f/MbVoUULt4/F5v33C9tHVRN6o4F5+OGGcfeZXqlhnPffV73wQrfs0ktVe/Rwnzt1Cg8H\nNTQTJrj6zZxZ2Hp27XLrOeqo4tRLVfWhh9w6+/Z1F5cNG/Jbj3e5l1zi3v/1r/zr9NprzoiMH+/2\nWdVd3M49N9r377rL1aFXr/zrkInq6vzvWjyZhN4GNTNKTrYG3spKl+2Trft9Mamqqp3OOXOmyw4B\nuOee5FCzGzZEe75vOiZNSp8+WgjpGmNzpVmz5DMMisW3vuUaLBctcr22O+Y56Mmpp7r9vO8+12gc\nzJjKldGj4frr3e9/zjmu49OKFdEaYiGZiRa1ITZXmjd3mUkla5BNdwVoyJc5+niTKdQT1fn7WHy6\nhtlcY/rZ1uPvADI1TvvPfp/C9icYNiqEa65x6ytGA+qAAap33134eoL8+teqbdqoLl9e2HrOOcft\n5+DBhdeppkb1V79y6zvgAPf+8svRvuvvLr72tcLrkY6LLnIhtnxDQ1joxmgq5CKkwfL1Ee7p3NnF\naHO5gEQJG+XDL3/p1lVVVfi6SoFvUC2U5593+xk1xBKFhx92jbCgumJFbvW49NLi1SOV+fNVZ8wo\njdBb6MZoVGQbwrmmxi0fP752eVUX/vG9FlPDQ8XoDxB8jGM2VNMv++ijaCGdTGX8qIj1/RCbqIgU\n3gcB3GP2Ro2q+6CRQhg/3g2K97//Gz0U4/PoSxW6ARgwwPW0LkXfFRN6IzaMH+96QarWHt5hv/1q\nj+7Z0Ki6MW7S9QgG93nChNplgs8ErqhwPWxfeaU0bQBBghec4Aimwc+l2naLFq5H7Te/Wdz1HnOM\ne8JTVFEtdYy+5KSz+g35stCNUQoaY/ZPWFgnSjgqXdtCsB/BZZeFh8HSdSwr9DfL1v6Qrm2mKbBi\nhWrr1oWnxpbyN8Bi9IbhCIpoJpFsyD4AhTYw57KNbBeAfH6DdG0oqfsVvDCka5uJIoa5ime2fh/p\n1lNo/nwpG+dVTegNI5RsJ3bYiVlREd5AnGmIhsZ+F5HpApDveqL2jm7ePPw3iyqGYcco00UsndiG\n1TWKCOdykcl00SyGuzehN4w8yedWO+w7jbGHcFN7hQl3Lr9rZWX6i5e/4IQJcLrjG3aB8heHsP9A\ntju11DvKXMkk9OKWNy5GjBihM2fObOhqGEbR8I2rwTHWRdwpHoVcypYL9fGbiLhMr0mT3KiqVVXZ\nt9u5sxs1M3isKytd5o4fnjsblZWuk6HPLotWV5mlqqEPorSsG8OoB4K9g4OZQFHG/0/NGkqXOlqK\ntLxUgg+nL/XDarJRTJFv3jx8fq9etTOgomx3/fq6T1rz01HTYYPDdReFdFa/IV8WujHKiagNlqnl\nszUmhmXdhG0j6itdR698ejSnC5U0xCtbBlMx65pL24dIbv8jCo3RA2OARcASYGLI8v8HLADmAa8C\n+wWW7QLmJF7PRdmeCb1RrtRHCmJYhku2C0DU1MlsF4pMw10UoyE414tYOnEvZeZT1IbqXHtQFyT0\nQHPg38D+QAtgLjAgpcxooDLx+TLg8cCyz7NtI/VlQm8Y9U8hKY7BdUTNXskn6ynKhSTsIpbLK1cH\nn89FIbUxOcqdXDYKFfrDgVcC0z8BfpKh/CHAm4FpE3rDKCOKeVcS9e4jkzCWIuOp0D4XqWGZYvxm\nhQr9mcB9genzgDsylL8D+GlguhqYCbwNfC3b9tSE3jCMLOQijOnuMnJNtWzePLe7j0zbKMbAdqlk\nEvqIjyKOhoicC4wAjgnM3k9VV4jI/sBrIvKuqv475LsTgAkAvXr1Kma1DMOIGePHR0899OWuucYN\nKNerF9x4o5uXmvJaWQkXXOAeWJ46P1O6Y67b8MvqjXRXAP8iYugG+CqwENgrw7oeBM7Mtk1z9IZh\n1Af5DodQjG0UGwrpMCUiewAfAMcBK4AZwDmqOj9Q5hDgSWCMqi4OzO8IbFXVL0SkC/BPYKyqLsi0\nTeswZRiGkRuZOkxlDd2oarWIfA94BZeBc7+qzheR63FXkOeAXwNtgb+I67XxkaqeDvQHfi8iNbjO\nWTdlE3nDMAyjuNgQCIZhGDHAhkAwDMMoY0zoDcMwYo4JvWEYRsxplDF6EVkLVOX59S7AuiJWpylQ\njvsM5bnf5bjPUJ77nes+76eqXcMWNEqhLwQRmZmuQSKulOM+Q3nudznuM5Tnfhdzny10YxiGEXNM\n6A3DMGJOHIX+3oauQANQjvsM5bnf5bjPUJ77XbR9jl2M3jAMw6hNHB29YRiGEcCE3jAMI+bERuhF\nZIyILBKRJSIysaHrUypEpKeITBWRBSIyX0SuSMzvJCJTRGRx4r1jQ9e12IhIcxH5l4j8NTHdR0Te\nSRzzx0WkRUPXsdiISAcReVJE3heRhSJyeNyPtYhcmfhvvycij4pIqzgeaxG5X0TWiMh7gXmhx1Yc\ntyf2f56IDM9lW7EQehFpDtwJnAQMAMaJyICGrVXJqAZ+pKoDgMOA7yb2dSLwqqoehHtAexwvdlfg\nnnng+V/gVlU9EPgUuKhBalVafgu8rKr9gKG4/Y/tsRaR7sAPgBGqOgg3Yu7ZxPNYPwiMSZmX7tie\nBByUeE0A7s5lQ7EQemAksERVl6rqDuAxYGwD16kkqOoqVZ2d+LwZd+J3x+3vnxLF/gR8rWFqWBpE\npAdwCnBfYlqAr+CegwDx3Of2wJeBPwKo6g5V/YyYH2vc8OmtE8/CqARWEcNjrarTgA0ps9Md27HA\nQ4lnjLwNdBCRfaNuKy5C3x34ODC9PDEv1ohIb9zD2N8B9lbVVYlFnwB7N1C1SsVtwH8BNYnpzsBn\nqlqdmI7jMe8DrAUeSISs7hORNsT4WKvqCuBm4COcwG8EZhH/Y+1Jd2wL0ri4CH3ZISJtgaeAH6rq\npuCyxGPFYpM3KyKnAmtUdVZD16We2QMYDtytqocAW0gJ08TwWHfEudc+QDegDXXDG2VBMY9tXIR+\nBdAzMN0jMS+WiEgFTuQnqerTidmr/a1c4n1NQ9WvBBwJnC4iy3Bhua/gYtcdErf3EM9jvhxYrqrv\nJKafxAl/nI/1V4EPVXWtqu4EnsYd/7gfa0+6Y1uQxsVF6GcAByVa5lvgGm+ea+A6lYREbPqPwEJV\nvSWw6DnggsTnC4Bn67tupUJVf6KqPVS1N+7Yvqaq44GpwJmJYrHaZwBV/QT4WET6JmYdBywgxsca\nF7I5TEQqE/91v8+xPtYB0h3b54DzE9k3hwEbAyGe7KR7anhTewEn4x5i/m/gmoauTwn38yjc7dw8\nYE7idTIuZv0qsBj4O9Cpoetaov0/Fvhr4vP+wHRgCfAXoGVD168E+zsMmJk43s8AHeN+rIH/Bt4H\n3gP+DLSM47EGHsW1Q+zE3b1dlO7YAoLLLPw38C4uKynytmwIBMMwjJgTl9CNYRiGkQYTesMwjJhj\nQm8YhhFzTOgNwzBijgm9YRhGzDGhNwzDiDkm9IZhGDHn/wO/9OXsd/bDpgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCfIgj5qrQGh",
        "colab_type": "code",
        "outputId": "39c4a1af-124b-4b4d-d93d-b07fadef1c71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "opt = SGD(lr=0.0001)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\"acc\"])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2,callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.88700\n",
            "100/100 - 19s - loss: 0.2357 - acc: 0.8975 - val_loss: 0.3640 - val_acc: 0.8760\n",
            "Epoch 2/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2359 - acc: 0.8985 - val_loss: 0.3856 - val_acc: 0.8730\n",
            "Epoch 3/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2249 - acc: 0.9150 - val_loss: 0.3767 - val_acc: 0.8760\n",
            "Epoch 4/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.88700\n",
            "100/100 - 16s - loss: 0.2337 - acc: 0.9080 - val_loss: 0.3750 - val_acc: 0.8790\n",
            "Epoch 5/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.2379 - acc: 0.9005 - val_loss: 0.3739 - val_acc: 0.8820\n",
            "Epoch 6/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.2343 - acc: 0.9100 - val_loss: 0.3735 - val_acc: 0.8850\n",
            "Epoch 7/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.2345 - acc: 0.9125 - val_loss: 0.3714 - val_acc: 0.8840\n",
            "Epoch 8/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.2133 - acc: 0.9110 - val_loss: 0.3707 - val_acc: 0.8840\n",
            "Epoch 9/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.88700\n",
            "100/100 - 17s - loss: 0.2027 - acc: 0.9100 - val_loss: 0.3704 - val_acc: 0.8820\n",
            "Epoch 10/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.88700\n",
            "100/100 - 19s - loss: 0.2234 - acc: 0.9145 - val_loss: 0.3664 - val_acc: 0.8860\n",
            "Epoch 11/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.88700\n",
            "100/100 - 18s - loss: 0.2198 - acc: 0.9090 - val_loss: 0.3717 - val_acc: 0.8850\n",
            "Epoch 12/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00012: val_acc improved from 0.88700 to 0.89000, saving model to /content/drive/My Drive/Colab Notebooks/weights.12-0.89.hdf5\n",
            "100/100 - 18s - loss: 0.2089 - acc: 0.9175 - val_loss: 0.3741 - val_acc: 0.8900\n",
            "Epoch 13/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.89000\n",
            "100/100 - 18s - loss: 0.2103 - acc: 0.9190 - val_loss: 0.3648 - val_acc: 0.8870\n",
            "Epoch 14/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.89000\n",
            "100/100 - 18s - loss: 0.1996 - acc: 0.9090 - val_loss: 0.3596 - val_acc: 0.8850\n",
            "Epoch 15/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00015: val_acc improved from 0.89000 to 0.89100, saving model to /content/drive/My Drive/Colab Notebooks/weights.15-0.89.hdf5\n",
            "100/100 - 18s - loss: 0.2117 - acc: 0.9160 - val_loss: 0.3670 - val_acc: 0.8910\n",
            "Epoch 16/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.89100\n",
            "100/100 - 17s - loss: 0.2070 - acc: 0.9210 - val_loss: 0.3695 - val_acc: 0.8870\n",
            "Epoch 17/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.89100\n",
            "100/100 - 17s - loss: 0.2151 - acc: 0.9160 - val_loss: 0.3593 - val_acc: 0.8880\n",
            "Epoch 18/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.89100\n",
            "100/100 - 17s - loss: 0.2045 - acc: 0.9105 - val_loss: 0.3630 - val_acc: 0.8880\n",
            "Epoch 19/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.89100\n",
            "100/100 - 17s - loss: 0.1924 - acc: 0.9245 - val_loss: 0.3638 - val_acc: 0.8900\n",
            "Epoch 20/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.89100\n",
            "100/100 - 19s - loss: 0.2056 - acc: 0.9205 - val_loss: 0.3630 - val_acc: 0.8850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y78b6NL92RBy",
        "colab_type": "code",
        "outputId": "1c4d51b7-1f41-4cf6-817e-228d0ada7c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "opt = Adam(lr=0.0005)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\"acc\"])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=20,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2,callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.89100\n",
            "100/100 - 20s - loss: 0.2415 - acc: 0.8990 - val_loss: 0.4981 - val_acc: 0.8510\n",
            "Epoch 2/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.89100\n",
            "100/100 - 18s - loss: 0.2594 - acc: 0.8975 - val_loss: 0.4518 - val_acc: 0.8710\n",
            "Epoch 3/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.89100\n",
            "100/100 - 18s - loss: 0.2067 - acc: 0.9125 - val_loss: 0.3530 - val_acc: 0.8870\n",
            "Epoch 4/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.89100\n",
            "100/100 - 18s - loss: 0.2208 - acc: 0.9080 - val_loss: 0.3909 - val_acc: 0.8690\n",
            "Epoch 5/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.89100\n",
            "100/100 - 18s - loss: 0.1962 - acc: 0.9165 - val_loss: 0.4477 - val_acc: 0.8630\n",
            "Epoch 6/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.89100\n",
            "100/100 - 18s - loss: 0.2160 - acc: 0.9125 - val_loss: 0.4568 - val_acc: 0.8540\n",
            "Epoch 7/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.89100\n",
            "100/100 - 18s - loss: 0.2388 - acc: 0.9015 - val_loss: 0.3324 - val_acc: 0.8740\n",
            "Epoch 8/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.89100\n",
            "100/100 - 17s - loss: 0.2088 - acc: 0.9030 - val_loss: 0.3598 - val_acc: 0.8750\n",
            "Epoch 9/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.89100\n",
            "100/100 - 17s - loss: 0.1823 - acc: 0.9225 - val_loss: 0.4064 - val_acc: 0.8780\n",
            "Epoch 10/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00010: val_acc improved from 0.89100 to 0.89900, saving model to /content/drive/My Drive/Colab Notebooks/weights.10-0.90.hdf5\n",
            "100/100 - 18s - loss: 0.1909 - acc: 0.9235 - val_loss: 0.3111 - val_acc: 0.8990\n",
            "Epoch 11/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.1912 - acc: 0.9225 - val_loss: 0.3971 - val_acc: 0.8640\n",
            "Epoch 12/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.89900\n",
            "100/100 - 16s - loss: 0.1778 - acc: 0.9280 - val_loss: 0.3822 - val_acc: 0.8890\n",
            "Epoch 13/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.2291 - acc: 0.9145 - val_loss: 0.3341 - val_acc: 0.8800\n",
            "Epoch 14/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.89900\n",
            "100/100 - 19s - loss: 0.1778 - acc: 0.9270 - val_loss: 0.3359 - val_acc: 0.8750\n",
            "Epoch 15/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.1754 - acc: 0.9315 - val_loss: 0.3924 - val_acc: 0.8540\n",
            "Epoch 16/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.1784 - acc: 0.9265 - val_loss: 0.3170 - val_acc: 0.8930\n",
            "Epoch 17/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00017: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1998 - acc: 0.9180 - val_loss: 0.3352 - val_acc: 0.8840\n",
            "Epoch 18/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1892 - acc: 0.9250 - val_loss: 0.3565 - val_acc: 0.8720\n",
            "Epoch 19/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1685 - acc: 0.9275 - val_loss: 0.3283 - val_acc: 0.8820\n",
            "Epoch 20/20\n",
            "Epoch 1/20\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1810 - acc: 0.9290 - val_loss: 0.3495 - val_acc: 0.8720\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iQPTtGf4PWR",
        "colab_type": "code",
        "outputId": "c8cda4e5-26a0-4a81-cda5-b5d5da6bacd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.optimizers import SGD\n",
        "opt = SGD(lr=0.0001)\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[\"acc\"])\n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=100,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=50,  # 1000 images = batch_size * steps\n",
        "      verbose=2,callbacks=[checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n",
            "/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py:735: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.89900\n",
            "100/100 - 19s - loss: 0.1778 - acc: 0.9265 - val_loss: 0.3319 - val_acc: 0.8840\n",
            "Epoch 2/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00002: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.1454 - acc: 0.9475 - val_loss: 0.3257 - val_acc: 0.8920\n",
            "Epoch 3/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1645 - acc: 0.9335 - val_loss: 0.3249 - val_acc: 0.8910\n",
            "Epoch 4/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.1490 - acc: 0.9415 - val_loss: 0.3263 - val_acc: 0.8900\n",
            "Epoch 5/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.1543 - acc: 0.9455 - val_loss: 0.3245 - val_acc: 0.8910\n",
            "Epoch 6/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1553 - acc: 0.9390 - val_loss: 0.3232 - val_acc: 0.8950\n",
            "Epoch 7/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1413 - acc: 0.9440 - val_loss: 0.3310 - val_acc: 0.8890\n",
            "Epoch 8/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1713 - acc: 0.9300 - val_loss: 0.3274 - val_acc: 0.8900\n",
            "Epoch 9/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00009: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1566 - acc: 0.9390 - val_loss: 0.3239 - val_acc: 0.8990\n",
            "Epoch 10/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00010: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1382 - acc: 0.9435 - val_loss: 0.3294 - val_acc: 0.8980\n",
            "Epoch 11/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00011: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1303 - acc: 0.9510 - val_loss: 0.3309 - val_acc: 0.8960\n",
            "Epoch 12/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00012: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1358 - acc: 0.9460 - val_loss: 0.3373 - val_acc: 0.8910\n",
            "Epoch 13/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00013: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1525 - acc: 0.9385 - val_loss: 0.3331 - val_acc: 0.8940\n",
            "Epoch 14/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00014: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1442 - acc: 0.9405 - val_loss: 0.3322 - val_acc: 0.8920\n",
            "Epoch 15/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00015: val_acc did not improve from 0.89900\n",
            "100/100 - 18s - loss: 0.1559 - acc: 0.9360 - val_loss: 0.3354 - val_acc: 0.8930\n",
            "Epoch 16/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00016: val_acc did not improve from 0.89900\n",
            "100/100 - 17s - loss: 0.1497 - acc: 0.9380 - val_loss: 0.3279 - val_acc: 0.8950\n",
            "Epoch 17/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00017: val_acc improved from 0.89900 to 0.90000, saving model to /content/drive/My Drive/Colab Notebooks/weights.17-0.90.hdf5\n",
            "100/100 - 17s - loss: 0.1677 - acc: 0.9360 - val_loss: 0.3230 - val_acc: 0.9000\n",
            "Epoch 18/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00018: val_acc did not improve from 0.90000\n",
            "100/100 - 17s - loss: 0.1386 - acc: 0.9515 - val_loss: 0.3290 - val_acc: 0.8980\n",
            "Epoch 19/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00019: val_acc did not improve from 0.90000\n",
            "100/100 - 18s - loss: 0.1540 - acc: 0.9395 - val_loss: 0.3295 - val_acc: 0.8940\n",
            "Epoch 20/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00020: val_acc did not improve from 0.90000\n",
            "100/100 - 18s - loss: 0.1471 - acc: 0.9430 - val_loss: 0.3274 - val_acc: 0.8970\n",
            "Epoch 21/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00021: val_acc did not improve from 0.90000\n",
            "100/100 - 18s - loss: 0.1635 - acc: 0.9275 - val_loss: 0.3221 - val_acc: 0.8990\n",
            "Epoch 22/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00022: val_acc did not improve from 0.90000\n",
            "100/100 - 17s - loss: 0.1420 - acc: 0.9455 - val_loss: 0.3300 - val_acc: 0.8960\n",
            "Epoch 23/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00023: val_acc improved from 0.90000 to 0.90100, saving model to /content/drive/My Drive/Colab Notebooks/weights.23-0.90.hdf5\n",
            "100/100 - 17s - loss: 0.1513 - acc: 0.9405 - val_loss: 0.3188 - val_acc: 0.9010\n",
            "Epoch 24/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00024: val_acc did not improve from 0.90100\n",
            "100/100 - 17s - loss: 0.1398 - acc: 0.9430 - val_loss: 0.3212 - val_acc: 0.9010\n",
            "Epoch 25/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00025: val_acc did not improve from 0.90100\n",
            "100/100 - 17s - loss: 0.1720 - acc: 0.9245 - val_loss: 0.3234 - val_acc: 0.9010\n",
            "Epoch 26/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00026: val_acc did not improve from 0.90100\n",
            "100/100 - 17s - loss: 0.1581 - acc: 0.9375 - val_loss: 0.3222 - val_acc: 0.8980\n",
            "Epoch 27/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00027: val_acc improved from 0.90100 to 0.90300, saving model to /content/drive/My Drive/Colab Notebooks/weights.27-0.90.hdf5\n",
            "100/100 - 18s - loss: 0.1365 - acc: 0.9490 - val_loss: 0.3220 - val_acc: 0.9030\n",
            "Epoch 28/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00028: val_acc did not improve from 0.90300\n",
            "100/100 - 18s - loss: 0.1391 - acc: 0.9460 - val_loss: 0.3266 - val_acc: 0.8960\n",
            "Epoch 29/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00029: val_acc did not improve from 0.90300\n",
            "100/100 - 17s - loss: 0.1475 - acc: 0.9410 - val_loss: 0.3277 - val_acc: 0.8980\n",
            "Epoch 30/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00030: val_acc did not improve from 0.90300\n",
            "100/100 - 18s - loss: 0.1479 - acc: 0.9370 - val_loss: 0.3250 - val_acc: 0.9000\n",
            "Epoch 31/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00031: val_acc did not improve from 0.90300\n",
            "100/100 - 18s - loss: 0.1543 - acc: 0.9355 - val_loss: 0.3318 - val_acc: 0.8940\n",
            "Epoch 32/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00032: val_acc did not improve from 0.90300\n",
            "100/100 - 18s - loss: 0.1438 - acc: 0.9395 - val_loss: 0.3283 - val_acc: 0.8980\n",
            "Epoch 33/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00033: val_acc did not improve from 0.90300\n",
            "100/100 - 18s - loss: 0.1539 - acc: 0.9330 - val_loss: 0.3335 - val_acc: 0.8910\n",
            "Epoch 34/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00034: val_acc did not improve from 0.90300\n",
            "100/100 - 18s - loss: 0.1297 - acc: 0.9420 - val_loss: 0.3350 - val_acc: 0.8970\n",
            "Epoch 35/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00035: val_acc did not improve from 0.90300\n",
            "100/100 - 18s - loss: 0.1539 - acc: 0.9440 - val_loss: 0.3400 - val_acc: 0.8920\n",
            "Epoch 36/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00036: val_acc improved from 0.90300 to 0.90500, saving model to /content/drive/My Drive/Colab Notebooks/weights.36-0.90.hdf5\n",
            "100/100 - 18s - loss: 0.1474 - acc: 0.9405 - val_loss: 0.3290 - val_acc: 0.9050\n",
            "Epoch 37/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00037: val_acc did not improve from 0.90500\n",
            "100/100 - 18s - loss: 0.1432 - acc: 0.9395 - val_loss: 0.3343 - val_acc: 0.9000\n",
            "Epoch 38/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00038: val_acc did not improve from 0.90500\n",
            "100/100 - 17s - loss: 0.1555 - acc: 0.9335 - val_loss: 0.3335 - val_acc: 0.8980\n",
            "Epoch 39/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00039: val_acc did not improve from 0.90500\n",
            "100/100 - 18s - loss: 0.1436 - acc: 0.9435 - val_loss: 0.3273 - val_acc: 0.9020\n",
            "Epoch 40/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00040: val_acc did not improve from 0.90500\n",
            "100/100 - 18s - loss: 0.1655 - acc: 0.9330 - val_loss: 0.3277 - val_acc: 0.8950\n",
            "Epoch 41/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00041: val_acc did not improve from 0.90500\n",
            "100/100 - 17s - loss: 0.1536 - acc: 0.9350 - val_loss: 0.3325 - val_acc: 0.8960\n",
            "Epoch 42/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00042: val_acc did not improve from 0.90500\n",
            "100/100 - 17s - loss: 0.1408 - acc: 0.9470 - val_loss: 0.3385 - val_acc: 0.8920\n",
            "Epoch 43/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00043: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1583 - acc: 0.9355 - val_loss: 0.3283 - val_acc: 0.8980\n",
            "Epoch 44/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00044: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1547 - acc: 0.9405 - val_loss: 0.3335 - val_acc: 0.8940\n",
            "Epoch 45/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00045: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1348 - acc: 0.9430 - val_loss: 0.3282 - val_acc: 0.9020\n",
            "Epoch 46/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00046: val_acc did not improve from 0.90500\n",
            "100/100 - 17s - loss: 0.1453 - acc: 0.9395 - val_loss: 0.3317 - val_acc: 0.8970\n",
            "Epoch 47/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00047: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1453 - acc: 0.9370 - val_loss: 0.3293 - val_acc: 0.9010\n",
            "Epoch 48/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00048: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1421 - acc: 0.9440 - val_loss: 0.3359 - val_acc: 0.8900\n",
            "Epoch 49/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00049: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1326 - acc: 0.9460 - val_loss: 0.3304 - val_acc: 0.9010\n",
            "Epoch 50/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00050: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1503 - acc: 0.9445 - val_loss: 0.3290 - val_acc: 0.8980\n",
            "Epoch 51/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00051: val_acc did not improve from 0.90500\n",
            "100/100 - 16s - loss: 0.1542 - acc: 0.9400 - val_loss: 0.3358 - val_acc: 0.8920\n",
            "Epoch 52/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00052: val_acc did not improve from 0.90500\n",
            "100/100 - 17s - loss: 0.1533 - acc: 0.9370 - val_loss: 0.3336 - val_acc: 0.8920\n",
            "Epoch 53/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00053: val_acc did not improve from 0.90500\n",
            "100/100 - 18s - loss: 0.1615 - acc: 0.9305 - val_loss: 0.3271 - val_acc: 0.8910\n",
            "Epoch 54/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00054: val_acc improved from 0.90500 to 0.90600, saving model to /content/drive/My Drive/Colab Notebooks/weights.54-0.91.hdf5\n",
            "100/100 - 18s - loss: 0.1369 - acc: 0.9430 - val_loss: 0.3220 - val_acc: 0.9060\n",
            "Epoch 55/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00055: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1538 - acc: 0.9365 - val_loss: 0.3235 - val_acc: 0.9010\n",
            "Epoch 56/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00056: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1424 - acc: 0.9425 - val_loss: 0.3301 - val_acc: 0.8930\n",
            "Epoch 57/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00057: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1426 - acc: 0.9445 - val_loss: 0.3260 - val_acc: 0.8980\n",
            "Epoch 58/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00058: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1502 - acc: 0.9400 - val_loss: 0.3226 - val_acc: 0.9060\n",
            "Epoch 59/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00059: val_acc did not improve from 0.90600\n",
            "100/100 - 16s - loss: 0.1471 - acc: 0.9420 - val_loss: 0.3306 - val_acc: 0.8960\n",
            "Epoch 60/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00060: val_acc did not improve from 0.90600\n",
            "100/100 - 19s - loss: 0.1596 - acc: 0.9385 - val_loss: 0.3316 - val_acc: 0.8970\n",
            "Epoch 61/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00061: val_acc did not improve from 0.90600\n",
            "100/100 - 18s - loss: 0.1526 - acc: 0.9435 - val_loss: 0.3269 - val_acc: 0.8960\n",
            "Epoch 62/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00062: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1445 - acc: 0.9415 - val_loss: 0.3255 - val_acc: 0.9020\n",
            "Epoch 63/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00063: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1501 - acc: 0.9390 - val_loss: 0.3277 - val_acc: 0.8940\n",
            "Epoch 64/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00064: val_acc did not improve from 0.90600\n",
            "100/100 - 18s - loss: 0.1320 - acc: 0.9460 - val_loss: 0.3224 - val_acc: 0.9050\n",
            "Epoch 65/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00065: val_acc did not improve from 0.90600\n",
            "100/100 - 18s - loss: 0.1464 - acc: 0.9440 - val_loss: 0.3304 - val_acc: 0.8960\n",
            "Epoch 66/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00066: val_acc did not improve from 0.90600\n",
            "100/100 - 18s - loss: 0.1323 - acc: 0.9480 - val_loss: 0.3358 - val_acc: 0.8960\n",
            "Epoch 67/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00067: val_acc did not improve from 0.90600\n",
            "100/100 - 18s - loss: 0.1406 - acc: 0.9455 - val_loss: 0.3296 - val_acc: 0.8990\n",
            "Epoch 68/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00068: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1549 - acc: 0.9405 - val_loss: 0.3278 - val_acc: 0.9000\n",
            "Epoch 69/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00069: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1529 - acc: 0.9440 - val_loss: 0.3259 - val_acc: 0.8960\n",
            "Epoch 70/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00070: val_acc did not improve from 0.90600\n",
            "100/100 - 18s - loss: 0.1499 - acc: 0.9395 - val_loss: 0.3280 - val_acc: 0.8970\n",
            "Epoch 71/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00071: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1563 - acc: 0.9360 - val_loss: 0.3304 - val_acc: 0.8910\n",
            "Epoch 72/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00072: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1425 - acc: 0.9470 - val_loss: 0.3201 - val_acc: 0.9050\n",
            "Epoch 73/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00073: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1320 - acc: 0.9485 - val_loss: 0.3249 - val_acc: 0.9010\n",
            "Epoch 74/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00074: val_acc did not improve from 0.90600\n",
            "100/100 - 17s - loss: 0.1351 - acc: 0.9435 - val_loss: 0.3300 - val_acc: 0.8960\n",
            "Epoch 75/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00075: val_acc improved from 0.90600 to 0.90700, saving model to /content/drive/My Drive/Colab Notebooks/weights.75-0.91.hdf5\n",
            "100/100 - 17s - loss: 0.1356 - acc: 0.9450 - val_loss: 0.3251 - val_acc: 0.9070\n",
            "Epoch 76/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00076: val_acc did not improve from 0.90700\n",
            "100/100 - 18s - loss: 0.1428 - acc: 0.9440 - val_loss: 0.3355 - val_acc: 0.8960\n",
            "Epoch 77/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00077: val_acc did not improve from 0.90700\n",
            "100/100 - 17s - loss: 0.1422 - acc: 0.9420 - val_loss: 0.3350 - val_acc: 0.8940\n",
            "Epoch 78/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00078: val_acc did not improve from 0.90700\n",
            "100/100 - 17s - loss: 0.1311 - acc: 0.9500 - val_loss: 0.3342 - val_acc: 0.8930\n",
            "Epoch 79/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00079: val_acc did not improve from 0.90700\n",
            "100/100 - 18s - loss: 0.1593 - acc: 0.9325 - val_loss: 0.3322 - val_acc: 0.8930\n",
            "Epoch 80/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00080: val_acc did not improve from 0.90700\n",
            "100/100 - 17s - loss: 0.1522 - acc: 0.9390 - val_loss: 0.3248 - val_acc: 0.9010\n",
            "Epoch 81/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00081: val_acc improved from 0.90700 to 0.90800, saving model to /content/drive/My Drive/Colab Notebooks/weights.81-0.91.hdf5\n",
            "100/100 - 18s - loss: 0.1469 - acc: 0.9390 - val_loss: 0.3193 - val_acc: 0.9080\n",
            "Epoch 82/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00082: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1582 - acc: 0.9375 - val_loss: 0.3248 - val_acc: 0.8980\n",
            "Epoch 83/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00083: val_acc did not improve from 0.90800\n",
            "100/100 - 18s - loss: 0.1493 - acc: 0.9465 - val_loss: 0.3163 - val_acc: 0.9060\n",
            "Epoch 84/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00084: val_acc did not improve from 0.90800\n",
            "100/100 - 18s - loss: 0.1482 - acc: 0.9410 - val_loss: 0.3237 - val_acc: 0.8980\n",
            "Epoch 85/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00085: val_acc did not improve from 0.90800\n",
            "100/100 - 18s - loss: 0.1372 - acc: 0.9465 - val_loss: 0.3206 - val_acc: 0.9070\n",
            "Epoch 86/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00086: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1421 - acc: 0.9450 - val_loss: 0.3233 - val_acc: 0.9020\n",
            "Epoch 87/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00087: val_acc did not improve from 0.90800\n",
            "100/100 - 18s - loss: 0.1397 - acc: 0.9450 - val_loss: 0.3276 - val_acc: 0.9010\n",
            "Epoch 88/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00088: val_acc did not improve from 0.90800\n",
            "100/100 - 18s - loss: 0.1485 - acc: 0.9430 - val_loss: 0.3288 - val_acc: 0.8980\n",
            "Epoch 89/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00089: val_acc did not improve from 0.90800\n",
            "100/100 - 18s - loss: 0.1428 - acc: 0.9405 - val_loss: 0.3345 - val_acc: 0.8940\n",
            "Epoch 90/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00090: val_acc did not improve from 0.90800\n",
            "100/100 - 16s - loss: 0.1377 - acc: 0.9430 - val_loss: 0.3254 - val_acc: 0.9000\n",
            "Epoch 91/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00091: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1416 - acc: 0.9420 - val_loss: 0.3284 - val_acc: 0.8950\n",
            "Epoch 92/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00092: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1297 - acc: 0.9465 - val_loss: 0.3290 - val_acc: 0.8950\n",
            "Epoch 93/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00093: val_acc did not improve from 0.90800\n",
            "100/100 - 16s - loss: 0.1485 - acc: 0.9350 - val_loss: 0.3282 - val_acc: 0.8990\n",
            "Epoch 94/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00094: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1259 - acc: 0.9530 - val_loss: 0.3343 - val_acc: 0.8950\n",
            "Epoch 95/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00095: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1486 - acc: 0.9415 - val_loss: 0.3365 - val_acc: 0.8930\n",
            "Epoch 96/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00096: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1409 - acc: 0.9420 - val_loss: 0.3249 - val_acc: 0.9020\n",
            "Epoch 97/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00097: val_acc did not improve from 0.90800\n",
            "100/100 - 16s - loss: 0.1508 - acc: 0.9470 - val_loss: 0.3285 - val_acc: 0.8950\n",
            "Epoch 98/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00098: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1469 - acc: 0.9415 - val_loss: 0.3248 - val_acc: 0.9010\n",
            "Epoch 99/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00099: val_acc did not improve from 0.90800\n",
            "100/100 - 17s - loss: 0.1338 - acc: 0.9455 - val_loss: 0.3318 - val_acc: 0.8950\n",
            "Epoch 100/100\n",
            "Epoch 1/100\n",
            "\n",
            "Epoch 00100: val_acc did not improve from 0.90800\n",
            "100/100 - 16s - loss: 0.1377 - acc: 0.9440 - val_loss: 0.3275 - val_acc: 0.9020\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tow8HXJnCQAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}